<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Note: basics for a neural network</title>
  <meta name="description" content="We will introduce basic components of a neural network briefly in this note.">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="/css/syntax-style.css">
  <link rel="stylesheet" href="/css/site.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
  <link rel="canonical" href="http://localhost:4000/2017/05/02/Note_basics-for-a-neural-network.html">
  <link rel="alternate" type="application/rss+xml" title="Wen-Ting Wang's Blog" href="http://localhost:4000/feed.xml" />

  <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
       <script type="text/x-mathjax-config">
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], ["\\(","\\)"] ],
             processEscapes: true
           }
         });
       </script>
       <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="/js/jquery-1.11.3.min.js"></script>
  <script type="text/javascript" src="/js/jq_mathjax_parse.js"></script>
  <script type="text/javascript" src="/js/jquery.toc.min.js"></script>

  
</head>


  <body>
    
    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Wen-Ting Wang's Blog</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
        
          
						<a class="page-link" href="/about/">About Me</a>
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <a class="page-link" href="/tags/python/">Python</a>
        <a class="page-link" href="/tags/R/">R</a>
        <a class="page-link" href="/tags/statistics/">Statistics</a>
        <a class="page-link" href="/tags/DL/">Deep learning</a>
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        


<div class="post">

  <header class="post-header">
    <h1 class="post-title">Note: basics for a neural network</h1>
    <p class="post-meta">May 2, 2017</p>
    <p id="post-meta"><i class="fa fa-tags"></i>: <a href="/tags/python/">Python</a>, <a href="/tags/deep learning/">Deep learning</a></p>
  </header>

  <article class="post-content">
    <p>We will introduce basic components of a neural network briefly in this note.</p>

<h2 id="1-activation-function">1. Activation function</h2>

<p>An “activation function” is a function applied at each node. Its purpose is to ensure that the representation in the input space is transformed to a different space in the output. That is, to produce a non-linear decision boundary based on non-linear combinations of the weighted inputs.</p>

<p>For example, a rectified linear unit  (reLU) function is defined as</p>
<div>
$$ f(x) = \max(0, x), x \in (-\infty, \infty); \label{eq:relu}$$
</div>
<p>a softplus function, a smoothed version of reLU, is</p>
<div>
$$f(x) = \log(1+e^x) \label{eq:softplus}$$
</div>
<p>where <span class="inlinecode">$x$</span> is the input to a neuron.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">reLU</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    
    <span class="c"># Calculate the value for the output of the reLU function: output</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
    
    <span class="c"># Return the value just calculated</span>
    <span class="k">return</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">softplus</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">exp</span><span class="p">,</span> <span class="n">log</span>
    <span class="c"># Calculate the value for the output of the softplus function: output</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
    
    <span class="c"># Return the value just calculated</span>
    <span class="k">return</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre>
</div>

<h4 id="visualization-of-relu-and-softplus">Visualization of reLU and softplus</h4>

<p>Let’s see the difference b/w (\ref{eq:relu}) and (\ref{eq:softplus}). We will consider <script type="math/tex">256</script> values of <script type="math/tex">x</script> from <script type="math/tex">-5</script> to <script type="math/tex">5</script> equally spaced.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">endpoint</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">a1</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">a2</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">256</span><span class="p">):</span>
    <span class="n">a1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reLU</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
    <span class="n">a2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">softplus</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"blue"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">"-"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"reLU"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"red"</span><span class="p">,</span>  <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">"-"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"softplus"</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'upper left'</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>  <span class="c"># gca stands for 'get current axis'</span>
<span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'right'</span><span class="p">]</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s">'none'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'top'</span><span class="p">]</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s">'none'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_ticks_position</span><span class="p">(</span><span class="s">'bottom'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'bottom'</span><span class="p">]</span><span class="o">.</span><span class="n">set_position</span><span class="p">((</span><span class="s">'data'</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_ticks_position</span><span class="p">(</span><span class="s">'left'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'left'</span><span class="p">]</span><span class="o">.</span><span class="n">set_position</span><span class="p">((</span><span class="s">'data'</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre>
</div>

<p><img src="http://localhost:4000/assets/note1_basics_files/note1_basics_3_0.png" alt="png" /></p>

<p>The above figure shows that the softplus (red line) is smoother than the reLU (blue line), especially when <script type="math/tex">x = [-2, 2]</script>.</p>

<h2 id="2-forward-propagation-in-one-layer">2. Forward propagation in one layer</h2>

<p>Let each of the <script type="math/tex">M</script> hidden layer nodes, <script type="math/tex">a_j</script>, be a linear combination of the input variables:</p>
<div>
$$
    a_j = \sum_{i = 1}^{p_1} \omega_{1ij}x_{i} + \theta_{1i}, \label{eq:hiddensum}
$$
</div>
<p>where <script type="math/tex">x_1, \dots, x_{p_1}</script> are <script type="math/tex">p_1</script> input variables, <script type="math/tex">\omega_{11j}, \dots, \omega_{1{p_1}j}</script> are <script type="math/tex">p_1</script> unknown parameters, and <script type="math/tex">\theta_{1j}</script> is an unknown bias node. The prediction with one hidden layer is</p>
<div>
$$
   \hat{y}_k = \tilde{f}\left(\sum_{j = 1}^M \gamma_{jk}\cdot f(a_j) + \beta_{j}\right), \label{eq:onelayer}
$$
</div>
<p>where <script type="math/tex">\gamma_{1k}, \dots, \gamma_{Mk}</script> are <script type="math/tex">M</script> unknown parameters, <script type="math/tex">\beta_{j}</script> is an unknown bias node, <script type="math/tex">f(\cdot)</script> and <script type="math/tex">\tilde{f}(\cdot)</script> are the activation functions for the hidden nodes and the response respectively, and <script type="math/tex">k = 1, \dots, n</script>.</p>

<h3 id="practice">Practice</h3>
<p>For the next example, we apply the reLU (\ref{eq:relu})and an identity function as <script type="math/tex">f(\cdot)</script> and <script type="math/tex">\tilde{f}(\cdot)</script>, respectively. The weights are given.</p>

<h4 id="define-a-prediction-function">Define a prediction function</h4>
<p>Here, we follow the above equation w/o bias parameters.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Define predict_with_network()</span>
<span class="k">def</span> <span class="nf">predict_with_one_layer</span><span class="p">(</span><span class="n">input_data_row</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>

    <span class="c"># Calculate node 0 value</span>
    <span class="n">node_0_input</span> <span class="o">=</span>  <span class="p">(</span><span class="n">input_data_row</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s">'node_0'</span><span class="p">])</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
    <span class="n">node_0_output</span> <span class="o">=</span> <span class="n">reLU</span><span class="p">(</span><span class="n">node_0_input</span><span class="p">)</span>

    <span class="c"># Calculate node 1 value</span>
    <span class="n">node_1_input</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_data_row</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s">'node_1'</span><span class="p">])</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
    <span class="n">node_1_output</span> <span class="o">=</span> <span class="n">reLU</span><span class="p">(</span><span class="n">node_1_input</span><span class="p">)</span>

    <span class="c"># Put node values into array: hidden_layer_outputs</span>
    <span class="n">hidden_layer_outputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">node_0_output</span><span class="p">,</span> <span class="n">node_1_output</span><span class="p">])</span>
    
    <span class="c"># Calculate model output</span>
    <span class="n">input_to_final_layer</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_layer_outputs</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s">'output'</span><span class="p">])</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
    <span class="n">model_output</span> <span class="o">=</span> <span class="n">reLU</span><span class="p">(</span><span class="n">input_to_final_layer</span><span class="p">)</span>
    
    <span class="c"># Return model output</span>
    <span class="k">return</span><span class="p">(</span><span class="n">model_output</span><span class="p">)</span>
</code></pre>
</div>

<h4 id="toy-example">Toy example</h4>
<p>Set the weights as <script type="math/tex">% <![CDATA[
{\bf{\omega}}_1 = \left[ \begin{array}{cc}
-5 & 5 \\
-1 & 1\\
\end{array} \right] %]]></script>, <script type="math/tex">{\bf{\gamma}} = \left[ \begin{array}{c}
3  \\
7\\
\end{array} \right]</script>, and the input data as <script type="math/tex">{\bf{x}} = \left[ \begin{array}{c}
7 \\
5\\
\end{array} \right]</script>. Use <code class="highlighter-rouge">predict_with_one_layer()</code> to calculate the prediciton as follows.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Example</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>
 <span class="s">'node_0'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">]),</span>
 <span class="s">'node_1'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">]),</span>
 <span class="s">'output'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">])}</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">predict_with_one_layer</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>30
</code></pre>
</div>

<h2 id="3-forward-propagation-in-two-layers">3. Forward propagation in two layers</h2>
<p>Similar to the prediction with one hidden layer (\ref{eq:hiddensum}, \ref{eq:onelayer}), the prediction with two hidden layers is</p>
<div>
$$
   \hat{y}_k = \tilde{f}\left(\sum_{j = 1}^M \gamma_{jk}\cdot f\left(\sum_{\ell = 1}^{p_2} \omega_{2\ell j} \cdot f(a_{\ell j}) + \theta_{2\ell}\right) + \beta_{j}\right),
$$
</div>
<p>where <script type="math/tex">\omega_{21j}, \dots, \omega_{2{p_2}j}</script> are <script type="math/tex">p_2</script> unknown parameters, <script type="math/tex">\theta_{2\ell}</script> is an unknown bias node.</p>

<h3 id="practice-1">Practice</h3>
<p>For the next example, we apply the reLU and an identity function as <script type="math/tex">f(\cdot)</script> and <script type="math/tex">\tilde{f}(\cdot)</script>, respectively. The weights are given.</p>

<h4 id="define-a-prediction-function-with-two-layers">Define a prediction function with two layers</h4>
<p>Here, we follow the above equation w/o bias parameters, and set <script type="math/tex">p_1 =p_2</script>.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict_with_two_layers</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="c"># Calculate node 0 in the first hidden layer</span>
    <span class="n">node_0_0_input</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_data</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s">'node_0_0'</span><span class="p">])</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
    <span class="n">node_0_0_output</span> <span class="o">=</span> <span class="n">reLU</span><span class="p">(</span><span class="n">node_0_0_input</span><span class="p">)</span>

    <span class="c"># Calculate node 1 in the first hidden layer</span>
    <span class="n">node_0_1_input</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_data</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s">'node_0_1'</span><span class="p">])</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
    <span class="n">node_0_1_output</span> <span class="o">=</span> <span class="n">reLU</span><span class="p">(</span><span class="n">node_0_1_input</span><span class="p">)</span>

    <span class="c"># Put node values into array: hidden_0_outputs</span>
    <span class="n">hidden_0_outputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">node_0_0_output</span><span class="p">,</span> <span class="n">node_0_1_output</span><span class="p">])</span>
    
    <span class="c"># Calculate node 0 in the second hidden layer</span>
    <span class="n">node_1_0_input</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_0_outputs</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s">'node_1_0'</span><span class="p">])</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
    <span class="n">node_1_0_output</span> <span class="o">=</span> <span class="n">reLU</span><span class="p">(</span><span class="n">node_1_0_input</span><span class="p">)</span>

    <span class="c"># Calculate node 1 in the second hidden layer</span>
    <span class="n">node_1_1_input</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_0_outputs</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s">'node_1_1'</span><span class="p">])</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
    <span class="n">node_1_1_output</span> <span class="o">=</span> <span class="n">reLU</span><span class="p">(</span><span class="n">node_1_1_input</span><span class="p">)</span>

    <span class="c"># Put node values into array: hidden_1_outputs</span>
    <span class="n">hidden_1_outputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">node_1_0_output</span><span class="p">,</span> <span class="n">node_1_1_output</span><span class="p">])</span>

    <span class="c"># Calculate model output: model_output</span>
    <span class="n">model_output</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_1_outputs</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="s">'output'</span><span class="p">])</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
    
    <span class="c"># Return model_output</span>
    <span class="k">return</span><span class="p">(</span><span class="n">model_output</span><span class="p">)</span>
</code></pre>
</div>

<h4 id="toy-example-1">Toy example</h4>
<p>Set the weights as <script type="math/tex">% <![CDATA[
{\bf{\omega}}_1 = \left[ \begin{array}{cc}
-5 & 5 \\
-1 & 3\\
\end{array} \right] %]]></script>, <script type="math/tex">% <![CDATA[
{\bf{\omega}}_2 = \left[ \begin{array}{cc}
-3 & 4 \\
2 & 2\\
\end{array} \right] %]]></script>, <script type="math/tex">{\bf{\gamma}} = \left[ \begin{array}{c}
2 \\
7\\
\end{array} \right]</script>, and the input data as <script type="math/tex">{\bf{x}} = \left[ \begin{array}{c}
7 \\
5\\
\end{array} \right]</script>. Use <code class="highlighter-rouge">predict_with_two_layer()</code> to calculate the prediciton as follows.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Example</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">weights</span> <span class="o">=</span> <span class="p">{</span><span class="s">'node_0_0'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">]),</span>
 <span class="s">'node_0_1'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span>
 <span class="s">'node_1_0'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">]),</span>
 <span class="s">'node_1_1'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span>
 <span class="s">'output'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">])}</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">predict_with_two_layers</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>176
</code></pre>
</div>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://www.datacamp.com/courses/deep-learning-in-python">DataCamp: Deep Learning in Python</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU and softplu</a></li>
  <li><a href="https://www.quora.com/What-is-the-role-of-the-activation-function-in-a-neural-network/answer/Sebastian-Raschka-1">Role of the activation function in a neural netword</a></li>
</ul>


    <div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'egpivo';
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

      <div class="footer-col  footer-col-1">
        <ul class="social-media-list">
           <li>
             <i class="fa fa-envelope"></i>
             <a href="mailto:egpivo@gmail.com">egpivo@gmail.com</a>
          </li>
          
          <li>
              <i class="fa fa-github"></i>
              <a href="https://github.com/egpivo"><span class="username">egpivo</span></a>
          </li>
          
          <li>
            <i class="fa fa-linkedin"></i>
            <a href="https://www.linkedin.com/in/wen-ting-wang-6083a17b/"><span class="username">wtwang</span></a>
          </li>
        </ul>
      </div>

      <div class="footer-col  footer-col-23">
        <p class="text">Science is a means whereby learning is achieved.
</p>
      </div>
    </div>
  </div>

</footer>


  </body>

</html>
