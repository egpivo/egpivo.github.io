<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Note: basics for a neural network</title>
  <meta name="description" content="We will introduce basic components of a neural network briefly in this note.">

  <link rel="icon" type="image/png" href="/assets/images/favicons/android-chrome-192x192.png" sizes="192x192">
  <link rel="icon" type="image/png" href="/assets/images/favicons/android-chrome-512x512.png" sizes="512x512">
  <link rel="icon" type="image/png" href="/assets/images/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="/assets/images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="apple-touch-icon" href="/assets/images/favicons/apple-touch-icon.png">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="/css/syntax-style.css">
  <link rel="stylesheet" href="/css/site.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
  <link rel="canonical" href="http://localhost:4000/2017/05/02/Note_basics-for-a-neural-network.html">
  <link rel="alternate" type="application/rss+xml" title="Wen-Ting Wang's Blog" href="http://localhost:4000/feed.xml" />

  <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
       <script type="text/x-mathjax-config">
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], ["\\(","\\)"] ],
             processEscapes: true
           }
         });
       </script>
       <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="/js/jquery-1.11.3.min.js"></script>
  <script type="text/javascript" src="/js/jq_mathjax_parse.js"></script>
  <script type="text/javascript" src="/js/jquery.toc.min.js"></script>

  
</head>


  <body>
    
    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Wen-Ting Wang's Blog</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
        
          
        
          
						<a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <a class="page-link" href="/tags/Python/">Python</a>
        <a class="page-link" href="/tags/R/">R</a>
        <a class="page-link" href="/tags/Statistics/">Statistics</a>

        <a class="page-link" href="/tags/ML/">Machine Learning</a>
                <a class="page-link" href="/tags/Software/">Software</a>
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        


<div class="post">

  <header class="post-header">
    <h1 class="post-title">Note: basics for a neural network</h1>
    <p class="post-meta">May 2, 2017</p>
    <p id="post-meta"><i class="fa fa-tags"></i>: <a href="/tags/Python/">Python</a>, <a href="/tags/ML/">ML</a></p>
  </header>

  <article class="post-content">
    <p>We will introduce basic components of a neural network briefly in this note.</p>

<h2 id="1-activation-function">1. Activation function</h2>

<p>An “activation function” is a function applied at each node. Its purpose is to ensure that the representation in the input space is transformed to a different space in the output. That is, to produce a non-linear decision boundary based on non-linear combinations of the weighted inputs.</p>

<p>For example, a rectified linear unit  (reLU) function is defined as</p>
<div>
$$ f(x) = \max(0, x), x \in (-\infty, \infty); \label{eq:relu}$$
</div>
<p>a softplus function, a smoothed version of reLU, is</p>
<div>
$$f(x) = \log(1+e^x) \label{eq:softplus}$$
</div>
<p>where <span class="inlinecode">$x$</span> is the input to a neuron.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">reLU</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>

<span class="c1"># Calculate the value for the output of the reLU function: output
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>

<span class="c1"># Return the value just calculated
</span><span class="nf">return</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">softplus</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="kn">from</span> <span class="n">math</span> <span class="kn">import</span> <span class="n">exp</span><span class="p">,</span> <span class="n">log</span>
<span class="c1"># Calculate the value for the output of the softplus function: output
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">log</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span> <span class="nf">exp</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>

<span class="c1"># Return the value just calculated
</span><span class="nf">return</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="visualization-of-relu-and-softplus">Visualization of reLU and softplus</h4>

<p>Let’s see the difference b/w (\ref{eq:relu}) and (\ref{eq:softplus}). We will consider \(256\) values of \(x\) from \(-5\) to \(5\) equally spaced.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">endpoint</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">a1</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">a2</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">256</span><span class="p">):</span>
<span class="n">a1</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">reLU</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
<span class="n">a2</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">softplus</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">reLU</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">,</span>  <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">softplus</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="sh">'</span><span class="s">upper left</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">()</span>  <span class="c1"># gca stands for 'get current axis'
</span><span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="sh">'</span><span class="s">right</span><span class="sh">'</span><span class="p">].</span><span class="nf">set_color</span><span class="p">(</span><span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="sh">'</span><span class="s">top</span><span class="sh">'</span><span class="p">].</span><span class="nf">set_color</span><span class="p">(</span><span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">xaxis</span><span class="p">.</span><span class="nf">set_ticks_position</span><span class="p">(</span><span class="sh">'</span><span class="s">bottom</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="sh">'</span><span class="s">bottom</span><span class="sh">'</span><span class="p">].</span><span class="nf">set_position</span><span class="p">((</span><span class="sh">'</span><span class="s">data</span><span class="sh">'</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
<span class="n">ax</span><span class="p">.</span><span class="n">yaxis</span><span class="p">.</span><span class="nf">set_ticks_position</span><span class="p">(</span><span class="sh">'</span><span class="s">left</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="sh">'</span><span class="s">left</span><span class="sh">'</span><span class="p">].</span><span class="nf">set_position</span><span class="p">((</span><span class="sh">'</span><span class="s">data</span><span class="sh">'</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="http://localhost:4000/assets/note1_basics_files/note1_basics_3_0.png" alt="png" /></p>

<p>The above figure shows that the softplus (red line) is smoother than the reLU (blue line), especially when \(x = [-2, 2]\).</p>

<h2 id="2-forward-propagation-in-one-layer">2. Forward propagation in one layer</h2>

<p>Let each of the \(M\) hidden layer nodes, \(a_j\), be a linear combination of the input variables:</p>
<div>
$$
a_j = \sum_{i = 1}^{p_1} \omega_{1ij}x_{i} + \theta_{1i}, \label{eq:hiddensum}
$$
</div>
<p>where \(x_1, \dots, x_{p_1}\) are \(p_1\) input variables, \(\omega_{11j}, \dots, \omega_{1{p_1}j}\) are \(p_1\) unknown parameters, and \(\theta_{1j}\) is an unknown bias node. The prediction with one hidden layer is</p>
<div>
$$
\hat{y}_k = \tilde{f}\left(\sum_{j = 1}^M \gamma_{jk}\cdot f(a_j) + \beta_{j}\right), \label{eq:onelayer}
$$
</div>
<p>where \(\gamma_{1k}, \dots, \gamma_{Mk}\) are \(M\) unknown parameters, \(\beta_{j}\) is an unknown bias node, \(f(\cdot)\) and \(\tilde{f}(\cdot)\) are the activation functions for the hidden nodes and the response respectively, and \(k = 1, \dots, n\).</p>

<h3 id="practice">Practice</h3>
<p>For the next example, we apply the reLU (\ref{eq:relu})and an identity function as \(f(\cdot)\) and \(\tilde{f}(\cdot)\), respectively. The weights are given.</p>

<h4 id="define-a-prediction-function">Define a prediction function</h4>
<p>Here, we follow the above equation w/o bias parameters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define predict_with_network()
</span><span class="k">def</span> <span class="nf">predict_with_one_layer</span><span class="p">(</span><span class="n">input_data_row</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>

<span class="c1"># Calculate node 0 value
</span><span class="n">node_0_input</span> <span class="o">=</span>  <span class="p">(</span><span class="n">input_data_row</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="sh">'</span><span class="s">node_0</span><span class="sh">'</span><span class="p">]).</span><span class="nf">sum</span><span class="p">()</span>
<span class="n">node_0_output</span> <span class="o">=</span> <span class="nf">reLU</span><span class="p">(</span><span class="n">node_0_input</span><span class="p">)</span>

<span class="c1"># Calculate node 1 value
</span><span class="n">node_1_input</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_data_row</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="sh">'</span><span class="s">node_1</span><span class="sh">'</span><span class="p">]).</span><span class="nf">sum</span><span class="p">()</span>
<span class="n">node_1_output</span> <span class="o">=</span> <span class="nf">reLU</span><span class="p">(</span><span class="n">node_1_input</span><span class="p">)</span>

<span class="c1"># Put node values into array: hidden_layer_outputs
</span><span class="n">hidden_layer_outputs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">node_0_output</span><span class="p">,</span> <span class="n">node_1_output</span><span class="p">])</span>

<span class="c1"># Calculate model output
</span><span class="n">input_to_final_layer</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_layer_outputs</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="sh">'</span><span class="s">output</span><span class="sh">'</span><span class="p">]).</span><span class="nf">sum</span><span class="p">()</span>
<span class="n">model_output</span> <span class="o">=</span> <span class="nf">reLU</span><span class="p">(</span><span class="n">input_to_final_layer</span><span class="p">)</span>

<span class="c1"># Return model output
</span><span class="nf">return</span><span class="p">(</span><span class="n">model_output</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="toy-example">Toy example</h4>
<p>Set the weights as \({\bf{\omega}}_1 = \left[ \begin{array}{cc}
-5 &amp; 5 \\
-1 &amp; 1\\
\end{array} \right]\), \({\bf{\gamma}} = \left[ \begin{array}{c}
3  \\
7\\
\end{array} \right]\), and the input data as \({\bf{x}} = \left[ \begin{array}{c}
7 \\
5\\
\end{array} \right]\). Use <code class="language-plaintext highlighter-rouge">predict_with_one_layer()</code> to calculate the prediciton as follows.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>
<span class="sh">'</span><span class="s">node_0</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">]),</span>
<span class="sh">'</span><span class="s">node_1</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">]),</span>
<span class="sh">'</span><span class="s">output</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">])}</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">output</span> <span class="o">=</span> <span class="nf">predict_with_one_layer</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre></div></div>

<p>30</p>

<h2 id="3-forward-propagation-in-two-layers">3. Forward propagation in two layers</h2>
<p>Similar to the prediction with one hidden layer (\ref{eq:hiddensum}, \ref{eq:onelayer}), the prediction with two hidden layers is</p>
<div>
$$
\hat{y}_k = \tilde{f}\left(\sum_{j = 1}^M \gamma_{jk}\cdot f\left(\sum_{\ell = 1}^{p_2} \omega_{2\ell j} \cdot f(a_{\ell j}) + \theta_{2\ell}\right) + \beta_{j}\right),
$$
</div>
<p>where \(\omega_{21j}, \dots, \omega_{2{p_2}j}\) are \(p_2\) unknown parameters, \(\theta_{2\ell}\) is an unknown bias node.</p>

<h3 id="practice-1">Practice</h3>
<p>For the next example, we apply the reLU and an identity function as \(f(\cdot)\) and \(\tilde{f}(\cdot)\), respectively. The weights are given.</p>

<h4 id="define-a-prediction-function-with-two-layers">Define a prediction function with two layers</h4>
<p>Here, we follow the above equation w/o bias parameters, and set \(p_1 =p_2\).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict_with_two_layers</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
<span class="c1"># Calculate node 0 in the first hidden layer
</span><span class="n">node_0_0_input</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_data</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="sh">'</span><span class="s">node_0_0</span><span class="sh">'</span><span class="p">]).</span><span class="nf">sum</span><span class="p">()</span>
<span class="n">node_0_0_output</span> <span class="o">=</span> <span class="nf">reLU</span><span class="p">(</span><span class="n">node_0_0_input</span><span class="p">)</span>

<span class="c1"># Calculate node 1 in the first hidden layer
</span><span class="n">node_0_1_input</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_data</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="sh">'</span><span class="s">node_0_1</span><span class="sh">'</span><span class="p">]).</span><span class="nf">sum</span><span class="p">()</span>
<span class="n">node_0_1_output</span> <span class="o">=</span> <span class="nf">reLU</span><span class="p">(</span><span class="n">node_0_1_input</span><span class="p">)</span>

<span class="c1"># Put node values into array: hidden_0_outputs
</span><span class="n">hidden_0_outputs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">node_0_0_output</span><span class="p">,</span> <span class="n">node_0_1_output</span><span class="p">])</span>

<span class="c1"># Calculate node 0 in the second hidden layer
</span><span class="n">node_1_0_input</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_0_outputs</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="sh">'</span><span class="s">node_1_0</span><span class="sh">'</span><span class="p">]).</span><span class="nf">sum</span><span class="p">()</span>
<span class="n">node_1_0_output</span> <span class="o">=</span> <span class="nf">reLU</span><span class="p">(</span><span class="n">node_1_0_input</span><span class="p">)</span>

<span class="c1"># Calculate node 1 in the second hidden layer
</span><span class="n">node_1_1_input</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_0_outputs</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="sh">'</span><span class="s">node_1_1</span><span class="sh">'</span><span class="p">]).</span><span class="nf">sum</span><span class="p">()</span>
<span class="n">node_1_1_output</span> <span class="o">=</span> <span class="nf">reLU</span><span class="p">(</span><span class="n">node_1_1_input</span><span class="p">)</span>

<span class="c1"># Put node values into array: hidden_1_outputs
</span><span class="n">hidden_1_outputs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">node_1_0_output</span><span class="p">,</span> <span class="n">node_1_1_output</span><span class="p">])</span>

<span class="c1"># Calculate model output: model_output
</span><span class="n">model_output</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_1_outputs</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="sh">'</span><span class="s">output</span><span class="sh">'</span><span class="p">]).</span><span class="nf">sum</span><span class="p">()</span>

<span class="c1"># Return model_output
</span><span class="nf">return</span><span class="p">(</span><span class="n">model_output</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="toy-example-1">Toy example</h4>
<p>Set the weights as \({\bf{\omega}}_1 = \left[ \begin{array}{cc}
-5 &amp; 5 \\
-1 &amp; 3\\
\end{array} \right]\), \({\bf{\omega}}_2 = \left[ \begin{array}{cc}
-3 &amp; 4 \\
2 &amp; 2\\
\end{array} \right]\), \({\bf{\gamma}} = \left[ \begin{array}{c}
2 \\
7\\
\end{array} \right]\), and the input data as \({\bf{x}} = \left[ \begin{array}{c}
7 \\
5\\
\end{array} \right]\). Use <code class="language-plaintext highlighter-rouge">predict_with_two_layer()</code> to calculate the prediction as follows.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">weights</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">node_0_0</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">]),</span>
<span class="sh">'</span><span class="s">node_0_1</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span>
<span class="sh">'</span><span class="s">node_1_0</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">]),</span>
<span class="sh">'</span><span class="s">node_1_1</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span>
<span class="sh">'</span><span class="s">output</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">])}</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">output</span> <span class="o">=</span> <span class="nf">predict_with_two_layers</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre></div></div>

<p>176</p>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://www.datacamp.com/courses/deep-learning-in-python">DataCamp: Deep Learning in Python</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU and softplu</a></li>
  <li><a href="https://www.quora.com/What-is-the-role-of-the-activation-function-in-a-neural-network/answer/Sebastian-Raschka-1">Role of the activation function in a neural netword</a></li>
</ul>


    <div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'egpivo';
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

      <div class="footer-col  footer-col-1">
        <ul class="social-media-list">
           <li>
             <i class="fa fa-envelope"></i>
             <a href="mailto:egpivo@gmail.com">egpivo@gmail.com</a>
          </li>
          
          <li>
              <i class="fa fa-github"></i>
              <a href="https://github.com/egpivo"><span class="username">egpivo</span></a>
          </li>
          
          <li>
            <i class="fa fa-linkedin"></i>
            <a href="https://www.linkedin.com/in/wtwang/"><span class="username">wtwang</span></a>
          </li>
        </ul>
      </div>

      <div class="footer-col  footer-col-23">
        <p class="text">Science is a means whereby learning is achieved.
</p>
      </div>
    </div>
  </div>

</footer>


  </body>

</html>
