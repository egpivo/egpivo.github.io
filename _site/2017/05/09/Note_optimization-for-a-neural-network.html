<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Note: optimization for a neural network</title>
  <meta name="description" content="Loss functionAfter fitting the model based on the certain weights, we can also calculate the prediction errors to see how accurate the predictions are to the...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="/css/syntax-style.css">
  <link rel="stylesheet" href="/css/site.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
  <link rel="canonical" href="http://localhost:4000/2017/05/09/Note_optimization-for-a-neural-network.html">
  <link rel="alternate" type="application/rss+xml" title="Wen-Ting Wang's Blog" href="http://localhost:4000/feed.xml" />

  <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
       <script type="text/x-mathjax-config">
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], ["\\(","\\)"] ],
             processEscapes: true
           }
         });
       </script>
       <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="/js/jquery-1.11.3.min.js"></script>
  <script type="text/javascript" src="/js/jq_mathjax_parse.js"></script>
  <script type="text/javascript" src="/js/jquery.toc.min.js"></script>

  
</head>


  <body>
    
    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Wen-Ting Wang's Blog</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
        
          
        
          
        
          
        
          
						<a class="page-link" href="/about/">About Me</a>
          
        
          
        
          
        
          
        
          
        
        <a class="page-link" href="/tags/Python/">Python</a>
        <a class="page-link" href="/tags/R/">R</a>
        <a class="page-link" href="/tags/Statistics/">Statistics</a>
        <a class="page-link" href="/tags/Deep learning/">Deep learning</a>
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        


<div class="post">

  <header class="post-header">
    <h1 class="post-title">Note: optimization for a neural network</h1>
    <p class="post-meta">May 9, 2017</p>
    <p id="post-meta"><i class="fa fa-tags"></i>: <a href="/tags/Python/">Python</a>, <a href="/tags/Deep learning/">Deep learning</a></p>
  </header>

  <article class="post-content">
    <h2 id="loss-function">Loss function</h2>
<p>After fitting the model based on the certain weights, we can also calculate the prediction errors to see how accurate the predictions are to the actual value. The prediction error is , <script type="math/tex">e_i({\bf{w}}) = y_i - \hat{y}_i({\bf{w}})</script>. However, how can we know the current predictions are the best ones? We can measure these by a loss function . For example, a squared error loss function is</p>
<div>
$$L_1({\bf{w}}) = \sum_{i=1}^n e^2_i({\bf{w}});$$
</div>
<p>an absolute error loss functionis</p>
<div>
$$L_2({\bf{w}}) = \sum_{i=1}^n \left|e_i({\bf{w}})\right|.$$
</div>
<p>That is, a lower loss function value means a better model performance.</p>

<h3 id="practice-toy-example">Practice: toy example</h3>

<p>Set two candidate sets of the weights as 
    - 1st set:</p>

<p><script type="math/tex">% <![CDATA[
{\bf{\omega}}_{11} = \left[ \begin{array}{cc}
-5 & 5 \\
-1 & 1\\
\end{array} \right] %]]></script>, <script type="math/tex">{\bf{\gamma}}_1 = \left[ \begin{array}{c}
3  \\
7\\
\end{array} \right]</script>
     - 2nd set:</p>

<p><script type="math/tex">% <![CDATA[
{\bf{\omega}}_{12} = \left[ \begin{array}{cc}
1 & 1 \\
2 & -1.5\\
\end{array} \right] %]]></script>, <script type="math/tex">{\bf{\gamma}}_2 = \left[ \begin{array}{c}
1.9  \\
3.5\\
\end{array} \right]</script>.</p>

<p>The input dataset is <script type="math/tex">% <![CDATA[
{\bf{X}}= \left[ \begin{array}{cc}
0 & 1\\
5 & 7\\
6 & -2\\
10 & 11\\
\end{array} \right] %]]></script>, and the response is  <script type="math/tex">{\bf{y}}= \left[ \begin{array}{c}
7\\
5\\
2\\
20\\
\end{array} \right]</script>. Use <code class="highlighter-rouge">predict_with_one_layer()</code> to calculate the predicitons, and the squared error loss function to measure the performance. The result below shows that the second candidate set of weights perfoms better than the first one.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="c"># import the functions in note1</span>
<span class="kn">from</span> <span class="nn">note1</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c"># Create model_output_1 </span>
<span class="n">model_output_1</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c"># Create model_output_2</span>
<span class="n">model_output_2</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">weights_1</span> <span class="o">=</span> <span class="p">{</span><span class="s">'node_0'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">]),</span>
            <span class="s">'node_1'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
            <span class="s">'output'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">])}</span>

<span class="n">weights_2</span> <span class="o">=</span> <span class="p">{</span><span class="s">'node_0'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
            <span class="s">'node_1'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">]),</span>
            <span class="s">'output'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.9</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">])}</span>

<span class="n">input_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
              <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">]),</span>
              <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">]),</span>
              <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">])]</span>

<span class="n">target_actuals</span> <span class="o">=</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">]</span>

<span class="c"># Loop over input_data</span>
<span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">input_data</span><span class="p">:</span>
    <span class="c"># Append prediction to model_output_0</span>
    <span class="n">model_output_1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">predict_with_one_layer</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">weights_1</span><span class="p">))</span>
    
    <span class="c"># Append prediction to model_output_1</span>
    <span class="n">model_output_2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">predict_with_one_layer</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">weights_2</span><span class="p">))</span>

<span class="c"># Calculate the mean squared error for model_output_0: mse_0</span>
<span class="n">mse_1</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">model_output_1</span><span class="p">,</span> <span class="n">target_actuals</span><span class="p">)</span>

<span class="c"># Calculate the mean squared error for model_output_1: mse_1</span>
<span class="n">mse_2</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">model_output_2</span><span class="p">,</span> <span class="n">target_actuals</span><span class="p">)</span>

<span class="c"># Print mse_0 and mse_1</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Mean squared error with weights_1: </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span><span class="n">mse_1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Mean squared error with weights_2: </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span><span class="n">mse_2</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Mean squared error with weights_1: 1779.500000
Mean squared error with weights_2: 1188.020625
</code></pre>
</div>

<ul>
  <li>Note: we can see that the sencond weight performs better.</li>
</ul>

<h2 id="optimization">Optimization</h2>
<p>Here, we want to find the weights that give the lowest value for the loss function.</p>

<h3 id="gradient-decent-gd">Gradient decent (GD)</h3>
<p>Simply, we can apply the gradient descent to address this problem. Gradient descent is a first-order iterative optimization algorithm, that is, the solution is computed along with the paths of the slope of loss function with respect to the weights. More detail can be found in <a href="https://en.wikipedia.org/wiki/Gradient_descent">here</a>.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="k">def</span> <span class="nf">gradientDescent</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">n_updates</span> <span class="o">=</span> <span class="mi">5</span><span class="p">):</span>
    
    <span class="c"># Iterate over the number of updates</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_updates</span><span class="p">):</span>
        <span class="c"># Calculate the predictions: preds</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="p">(</span><span class="n">weights</span> <span class="o">*</span> <span class="n">input_data</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
        <span class="c"># Calculate the error: error</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">target</span> <span class="o">-</span> <span class="n">preds</span>
        <span class="c"># Calculate the slope: slope</span>
        <span class="n">slope</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">input_data</span> <span class="o">*</span> <span class="n">error</span>
        <span class="c"># Update the weights: weights</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">+</span> <span class="n">slope</span> <span class="o">*</span> <span class="n">learning_rate</span>

        <span class="n">mse</span> <span class="o">=</span> <span class="p">(</span><span class="n">error</span><span class="p">)</span> <span class="o">**</span><span class="mi">2</span>
          
        <span class="k">print</span><span class="p">(</span><span class="s">"Iteration </span><span class="si">%</span><span class="s">d -- loss: </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">mse</span><span class="p">))</span>
     
    <span class="k">return</span> <span class="n">weights</span>
</code></pre>
</div>

<h3 id="practice-toy-example-1">Practice: toy example</h3>

<p>Set a initial vector of weights as <br />
${\bf{\omega}} = \left[ \begin{array}{c}
0 <br />
0 <br />
0
\end{array} \right]$, the input dataset is ${\bf{X}}= \left[ \begin{array}{c}
3<br />
1<br />
5
\end{array} \right]$, and the response is  $ y=8$. Use <code class="highlighter-rouge">gradientDescent()</code> to calculate the estimation of weights.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>     
<span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">target</span> <span class="o">=</span> <span class="o">-</span><span class="mi">8</span>
<span class="n">new_weight</span> <span class="o">=</span> <span class="n">gradientDescent</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">new_weight</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Iteration 1 -- loss: 64.000000
Iteration 2 -- loss: 5.760000
Iteration 3 -- loss: 0.518400
Iteration 4 -- loss: 0.046656
Iteration 5 -- loss: 0.004199
[-0.684048 -0.228016 -1.14008 ]
</code></pre>
</div>

<ul>
  <li>Note: we can see that the loss decreases as the number of iteration gets larger.</li>
</ul>

<h3 id="stochastic-gradient-descent-sgd">Stochastic gradient descent (SGD)</h3>
<ul>
  <li>The process of SGD is:
    <ol>
      <li>It is common to calculate slopes on only a subset of the randomly shuffled data (‘batch’)</li>
      <li>Use a different batch of data to calculate the next update</li>
      <li>Start over from the beginning once all data is used</li>
    </ol>
  </li>
  <li>The algorithm:
    <ol>
      <li>Randomly shuffle the data</li>
      <li>Split m subsets</li>
      <li>Do{<br />
  <script type="math/tex">\quad</script> for i = 1, …, m:
         <script type="math/tex">\omega_i</script> := <script type="math/tex">\omega_i -</script> learning rate * slope<br />
} until convergence</li>
    </ol>
  </li>
</ul>

<p>Remark: SGD usually converges faster than GD with a mild convergence rate. More detail can be found <a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf">here</a>.</p>

<h2 id="backpropagation">Backpropagation</h2>

<p>The process of backpropagation is:</p>
<ol>
  <li>Start at some random set of weights</li>
  <li>Use forward propagation to make a prediction</li>
  <li>Use backward propagation to calculate the slope of the loss function w.r.t each weight</li>
  <li>Multiply that slope by the learning rate, and subtract from the current weights</li>
  <li>Keep going with that cycle until we get to a flat part</li>
</ol>

<p>Remark: More detail can be found <a href="https://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf">here</a>.</p>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://www.datacamp.com/courses/deep-learning-in-python">DataCamp: Deep Learning in Python</a></li>
  <li><a href="https://www.quora.com/Whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent/answer/Sebastian-Raschka-1">What’s the difference between gradient descent and stochastic gradient descent?</a>
*<a href="https://www.quora.com/What-is-the-best-visual-explanation-for-the-back-propagation-algorithm-for-neural-networks/answer/Sebastian-Raschka-1">What is the best visual explanation for the back propagation algorithm for neural networks?</a></li>
</ul>


    <div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'egpivo';
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

      <div class="footer-col  footer-col-1">
        <ul class="social-media-list">
           <li>
             <i class="fa fa-envelope"></i>
             <a href="mailto:egpivo@gmail.com">egpivo@gmail.com</a>
          </li>
          
          <li>
              <i class="fa fa-github"></i>
              <a href="https://github.com/egpivo"><span class="username">egpivo</span></a>
          </li>
          
          <li>
            <i class="fa fa-linkedin"></i>
            <a href="https://www.linkedin.com/in/wen-ting-wang-6083a17b/"><span class="username">wtwang</span></a>
          </li>
        </ul>
      </div>

      <div class="footer-col  footer-col-23">
        <p class="text">Science is a means whereby learning is achieved.
</p>
      </div>
    </div>
  </div>

</footer>


  </body>

</html>
