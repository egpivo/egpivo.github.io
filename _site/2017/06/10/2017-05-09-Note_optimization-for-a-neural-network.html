<h2 id="loss-function">Loss function</h2>
<p>After fitting the model based on the certain weights, we can also calculate the prediction errors to see how accurate the predictions are to the actual value. The prediction error is , <script type="math/tex">e_i({\bf{w}}) = y_i - \hat{y}_i({\bf{w}})</script>. However, how can we know the current predictions are the best ones? We can measure these by a loss function . For example, a squared error loss function is</p>
<div>
$$L_1({\bf{w}}) = \sum_{i=1}^n e^2_i({\bf{w}});$$
</div>
<p>an absolute error loss functions</p>
<div>
$$L_2({\bf{w}}) = \sum_{i=1}^n \left|e_i({\bf{w}})\right|.$$
</div>
<p>That is, a lower loss function value means a better model performance.</p>

<h3 id="practice-toy-example">Practice: toy example</h3>

<p>Set two candidate sets of the weights as</p>
<ul>
  <li>1st set:</li>
</ul>

<p><script type="math/tex">% <![CDATA[
{\bf{\omega}}_{11} = \left[ \begin{array}{cc}
-5 & 5 \\
-1 & 1\\
\end{array} \right] %]]></script>, <script type="math/tex">{\bf{\gamma}}_1 = \left[ \begin{array}{c}
3  \\
7\\
\end{array} \right]</script></p>
<ul>
  <li>2nd set:</li>
</ul>

<p><script type="math/tex">% <![CDATA[
{\bf{\omega}}_{12} = \left[ \begin{array}{cc}
1 & 1 \\
2 & -1.5\\
\end{array} \right] %]]></script>, <script type="math/tex">{\bf{\gamma}}_2 = \left[ \begin{array}{c}
1.9  \\
3.5\\
\end{array} \right]</script>.</p>

<p>The input dataset is <script type="math/tex">% <![CDATA[
{\bf{X}}= \left[ \begin{array}{cc}
0 & 1\\
5 & 7\\
6 & -2\\
10 & 11\\
\end{array} \right] %]]></script>, and the response is  <script type="math/tex">{\bf{y}}= \left[ \begin{array}{c}
7\\
5\\
2\\
20\\
\end{array} \right]</script>. Use <code class="highlighter-rouge">predict_with_one_layer()</code> to calculate the predictions, and the squared error loss function to measure the performance. The result below shows that the second candidate set of weights performs better than the first one.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="c"># import the functions in note1</span>
<span class="kn">from</span> <span class="nn">note1</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c"># Create model_output_1 </span>
<span class="n">model_output_1</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c"># Create model_output_2</span>
<span class="n">model_output_2</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">weights_1</span> <span class="o">=</span> <span class="p">{</span><span class="s">'node_0'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">]),</span>
<span class="s">'node_1'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
<span class="s">'output'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">])}</span>

<span class="n">weights_2</span> <span class="o">=</span> <span class="p">{</span><span class="s">'node_0'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
<span class="s">'node_1'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">]),</span>
<span class="s">'output'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.9</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">])}</span>

<span class="n">input_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
<span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">]),</span>
<span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">]),</span>
<span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">])]</span>

<span class="n">target_actuals</span> <span class="o">=</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">]</span>

<span class="c"># Loop over input_data</span>
<span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">input_data</span><span class="p">:</span>
<span class="c"># Append prediction to model_output_0</span>
<span class="n">model_output_1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">predict_with_one_layer</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">weights_1</span><span class="p">))</span>

<span class="c"># Append prediction to model_output_1</span>
<span class="n">model_output_2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">predict_with_one_layer</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">weights_2</span><span class="p">))</span>

<span class="c"># Calculate the mean squared error for model_output_0: mse_0</span>
<span class="n">mse_1</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">model_output_1</span><span class="p">,</span> <span class="n">target_actuals</span><span class="p">)</span>

<span class="c"># Calculate the mean squared error for model_output_1: mse_1</span>
<span class="n">mse_2</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">model_output_2</span><span class="p">,</span> <span class="n">target_actuals</span><span class="p">)</span>

<span class="c"># Print mse_0 and mse_1</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Mean squared error with weights_1: </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span><span class="n">mse_1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Mean squared error with weights_2: </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span><span class="n">mse_2</span><span class="p">)</span>
</code></pre>
</div>

<p>Mean squared error with weights_1: 1779.500000
Mean squared error with weights_2: 1188.020625</p>

<ul>
  <li>Note: we can see that the second weight performs better.</li>
</ul>

<h2 id="optimization">Optimization</h2>
<p>Here, we want to find the weights that give the lowest value for the loss function.</p>

<h3 id="gradient-decent-gd">Gradient decent (GD)</h3>
<p>Simply, we can apply the gradient descent to address this problem. Gradient descent is a first-order iterative optimization algorithm, that is, the solution is computed along with the paths of the slope of loss function with respect to the weights. More detail can be found in <a href="https://en.wikipedia.org/wiki/Gradient_descent">here</a>.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="k">def</span> <span class="nf">gradientDescent</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">n_updates</span> <span class="o">=</span> <span class="mi">5</span><span class="p">):</span>

<span class="c"># Iterate over the number of updates</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_updates</span><span class="p">):</span>
<span class="c"># Calculate the predictions: preds</span>
<span class="n">preds</span> <span class="o">=</span> <span class="p">(</span><span class="n">weights</span> <span class="o">*</span> <span class="n">input_data</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
<span class="c"># Calculate the error: error</span>
<span class="n">error</span> <span class="o">=</span> <span class="n">target</span> <span class="o">-</span> <span class="n">preds</span>
<span class="c"># Calculate the slope: slope</span>
<span class="n">slope</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">input_data</span> <span class="o">*</span> <span class="n">error</span>
<span class="c"># Update the weights: weights</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">+</span> <span class="n">slope</span> <span class="o">*</span> <span class="n">learning_rate</span>

<span class="n">mse</span> <span class="o">=</span> <span class="p">(</span><span class="n">error</span><span class="p">)</span> <span class="o">**</span><span class="mi">2</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Iteration </span><span class="si">%</span><span class="s">d -- loss: </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">mse</span><span class="p">))</span>

<span class="k">return</span> <span class="n">weights</span>
</code></pre>
</div>

<h3 id="practice-toy-example-1">Practice: toy example</h3>

<p>Set a initial vector of weights as <br />
${\bf{\omega}} = \left[ \begin{array}{c}
0 <br />
0 <br />
0
\end{array} \right]$, the input dataset is ${\bf{X}}= \left[ \begin{array}{c}
3<br />
1<br />
5
\end{array} \right]$, and the response is  $ y=8$. Use <code class="highlighter-rouge">gradientDescent()</code> to calculate the estimation of weights.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>     
<span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">target</span> <span class="o">=</span> <span class="o">-</span><span class="mi">8</span>
<span class="n">new_weight</span> <span class="o">=</span> <span class="n">gradientDescent</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">new_weight</span><span class="p">)</span>
</code></pre>
</div>

<p>Iteration 1 – loss: 64.000000
Iteration 2 – loss: 5.760000
Iteration 3 – loss: 0.518400
Iteration 4 – loss: 0.046656
Iteration 5 – loss: 0.004199
[-0.684048 -0.228016 -1.14008 ]</p>

<ul>
  <li>Note: we can see that the loss decreases as the number of iteration gets larger.</li>
</ul>

<h3 id="stochastic-gradient-descent-sgd">Stochastic gradient descent (SGD)</h3>
<ul>
  <li>The process of SGD is:
    <ol>
      <li>It is common to calculate slopes on only a subset of the randomly shuffled data (‘batch’)</li>
      <li>Use a different batch of data to calculate the next update</li>
      <li>Start over from the beginning once all data is used</li>
    </ol>
  </li>
  <li>The algorithm:
    <ol>
      <li>Randomly shuffle the data</li>
      <li>Split m subsets</li>
      <li>Do{<br />
<script type="math/tex">\quad</script> for i = 1, …, m:
<script type="math/tex">\omega_i</script> := <script type="math/tex">\omega_i -</script> learning rate * slope<br />
} until convergence</li>
    </ol>
  </li>
</ul>

<p>Remark: SGD usually converges faster than GD with a mild convergence rate. More detail can be found <a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf">here</a>.</p>

<h2 id="backpropagation">Backpropagation</h2>

<p>The process of backpropagation is:</p>
<ol>
  <li>Start at some random set of weights</li>
  <li>Use forward propagation to make a prediction</li>
  <li>Use backward propagation to calculate the slope of the loss function w.r.t each weight</li>
  <li>Multiply that slope by the learning rate, and subtract from the current weights</li>
  <li>Keep going with that cycle until we get to a flat part</li>
</ol>

<p>Remark: More detail can be found <a href="https://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf">here</a>.</p>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://www.datacamp.com/courses/deep-learning-in-python">DataCamp: Deep Learning in Python</a></li>
  <li><a href="https://www.quora.com/Whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent/answer/Sebastian-Raschka-1">What’s the difference between gradient descent and stochastic gradient descent?</a>
*<a href="https://www.quora.com/What-is-the-best-visual-explanation-for-the-back-propagation-algorithm-for-neural-networks/answer/Sebastian-Raschka-1">What is the best visual explanation for the back propagation algorithm for neural networks?</a></li>
</ul>
