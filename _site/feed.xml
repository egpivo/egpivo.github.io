<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Wen-Ting Wang's Blog</title>
    <description>Science is a means whereby learning is achieved.
</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 07 May 2017 23:14:26 +0800</pubDate>
    <lastBuildDate>Sun, 07 May 2017 23:14:26 +0800</lastBuildDate>
    <generator>Jekyll v3.4.3</generator>
    
      <item>
        <title>How to Do Bayesian Inference 101</title>
        <description>&lt;p&gt;Towards the end of the post &lt;a href=&quot;/2016/04/21/bayes-rule.html&quot;&gt;Bayes’ Rule&lt;/a&gt;, I eluded a bit to how Bayes’ rule becomes extremely powerful in Bayesian inference. The link happens when we start to interpret the variables of Bayes’ rule as parameters (&lt;span class=&quot;inlinecode&quot;&gt;$\theta$&lt;/span&gt;) of a model and observed data (&lt;span class=&quot;inlinecode&quot;&gt;$D$&lt;/span&gt;):&lt;/p&gt;

&lt;div&gt;
$$\begin{align}
P(X\ |\ Y) &amp;amp;= \frac{P(Y\ |\ X)\ P(X)}{P(Y)} \\
P(\theta\ |\ D) &amp;amp;= \frac{P(D\ |\ \theta)\ P(\theta)}{P(D)}
\end{align}$$
&lt;/div&gt;

&lt;p&gt;In this post, we will learn exactly how Bayes’ rule is used in Bayesian inference by going through a specific example of coin tossing. A lot of this post and examples are inspired by &lt;a href=&quot;https://sites.google.com/site/doingbayesiandataanalysis/&quot;&gt;John K. Kruschke’s “Doing Bayesian Data Analysis”&lt;/a&gt;. An incredible book that I have been using for my entry into world of Bayesian statistics. I would highly recommend anyone interested in Bayesian statistics to get this book.&lt;/p&gt;

&lt;div class=&quot;alert alert-dismissible alert-warning&quot;&gt;
&lt;h4&gt;Important!&lt;/h4&gt;
Please ensure that you are familiar with Bayes' rule before continuing as this post will not make sense with a thorough understanding of it. If you are not familiar, please review my previous post on &lt;a href=&quot;/2016/04/21/bayes-rule.html&quot;&gt;Bayes' Rule&lt;/a&gt; for details.
&lt;/div&gt;

&lt;p&gt;Here is an overview of what will be discussed in this post.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;

&lt;ul data-toc=&quot;body&quot; data-toc-headings=&quot;h2,h3&quot;&gt;&lt;/ul&gt;

&lt;h2 id=&quot;the-coin-flipping-example&quot;&gt;The Coin Flipping Example&lt;/h2&gt;

&lt;p&gt;Consider the scenario where you found a coin on the side of a street that had an odd looking geometry, unlike anything you have ever seen before. It still has two sides (heads and a tail), and you start to wonder:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;What is probability of getting a head on a given flip with this coin?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Given your knowledge of how a typical coin is, your prior guess is that is should be probably 0.5. But given the strange looking geometry, you also entertain the idea that it could be something like 0.4 or 0.6, but think these values are less probable than 0.5. You then proceed to flip the coin 100 times (because you are really bored and have time on your hands) and you notice that you get 83 heads. Given this observed data now, what is your guess at the probability of getting a head on a given flip?&lt;/p&gt;

&lt;p&gt;If we think back to our examples from the Bayes’ Rule post, we can see that this particular example is not that dissimilar:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We have a prior belief of what the probability of getting a heads is (0.5).&lt;/li&gt;
  &lt;li&gt;We have some observed data, which is 83 heads out of 100 coin tosses.&lt;/li&gt;
  &lt;li&gt;We need to update our belief of this probability now that we have some observed data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There is however one key difference in this particular example compared to our previous examples. &lt;strong&gt;Our prior is not a single fixed value, but rather a series of different possible values. It could be 0.5 or 0.4 or 0.6….in fact it could be any value between 0 and 1!&lt;/strong&gt; Moreover, the possible values are not all equally likely. For instance, we have a strong belief that it could be 0.5 (because of what we know about coins in general), and while 0.4 and 0.6 or any other value is possible we still think 0.5 is more probable. Visually, we could have something like:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;dplyr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ggplot2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;cd&quot;&gt;#' Generates a &quot;Triangle&quot; Prior Probability Distribution
#'
#' @param vals Sample space of all possible parameter values.
#' @return 2 column data.frame containing the parameter and its corresponding
#'   prior probability.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_prior_distr&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vals.pmin&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pmin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Normalize the prior so that they sum to 1.
&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dplyr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prior&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vals.pmin&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vals.pmin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Define the Space of all theta values
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.vals&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.prior.distr.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_prior_distr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;cd&quot;&gt;#' Plots the Prior Probability Distribution
#'
#' @param prior.distr.df Prior probability distribution data.frame from 
#'   get_prior_distr().
#' @param plot.x.labels Plot the parameter values on the x-axes that are taken
#'  from the input data.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_prior_distr&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prior.distr.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot.x.labels&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.prior.p&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prior.distr.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prior&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_segment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xend&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prior&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yend&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;expression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;expression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;paste&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;P(&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggtitle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Prior Distribution&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 

  &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot.x.labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.vals&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prior.distr.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;theta&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.prior.p&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
      &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.prior.p&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
      &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scale_x_continuous&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;breaks&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                         &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.prior.p&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_prior_distr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.prior.distr.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/how-to-bayesian-infer-101/prior-distr-1..svg&quot; alt=&quot;plot of chunk prior-distr&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here we have 10 different possibilities of theta and their associated probabilities. In fact, what we have just described is a probability distribution that was defined in the &lt;a href=&quot;/2016/04/21/bayes-rule.html&quot;&gt;Bayes’ Rule&lt;/a&gt;! So if we think of our prior as a random variable, &lt;span class=&quot;inlinecode&quot;&gt;$\theta$&lt;/span&gt;, then what we actually have is a prior probability distribution. What implications does this have? Well this ends up affecting how we measure the likelihood because before our likelihood was based on a single prior value. Instead, our likelihood essentially becomes a function of a series of candidate &lt;span class=&quot;inlinecode&quot;&gt;$\theta$&lt;/span&gt; values. In other words, the probability of seeing the observed data is different depending on what the value of &lt;span class=&quot;inlinecode&quot;&gt;$\theta$&lt;/span&gt; is.&lt;/p&gt;

&lt;h2 id=&quot;steps-of-bayesian-inference&quot;&gt;Steps of Bayesian Inference&lt;/h2&gt;

&lt;p&gt;Now that we have given this simple example of a situation, we can walk through an example of how we can answer the aforementioned question (i.e. What is probability of getting a head on a given flip with this coin?) in a Bayesian way. To do any Bayesian inference, we follow a 4 step process:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Identify the observed data you are working with.&lt;/li&gt;
  &lt;li&gt;Construct a probabilistic model to represent the data (likelihood).&lt;/li&gt;
  &lt;li&gt;Specify prior distributions over the parameters of your probabilistic model (prior).&lt;/li&gt;
  &lt;li&gt;Collect data and apply Bayes’ rule to re-allocate credibility across the possible parameter values (posterior).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let us walk through these steps one by one in the context of this example.&lt;/p&gt;

&lt;h3 id=&quot;step-1-identify-the-observed-data&quot;&gt;Step 1: Identify the Observed Data&lt;/h3&gt;

&lt;p&gt;In this example, we have a coin that when flipped gives us one of two possible outcomes: heads or tails. We can use the random variable, Y, to denote the outcome and assign it to 1 if it is heads and 0 if it is tails. These values are not ordinal, and merely represent a simplified way to represent heads or tails.&lt;/p&gt;

&lt;h3 id=&quot;step-2-construct-a-probabilistic-model-to-represent-the-data&quot;&gt;Step 2: Construct a Probabilistic Model to Represent the Data&lt;/h3&gt;

&lt;p&gt;Once we have identified the observed data, our next step is to come up with a probabilistic model along with meaningful parameters that can properly describe this data. The idea here is that the model is supposed to represent some process that generated this data (i.e. generative process). For this particular example, we can represent the probability of getting a head as &lt;span class=&quot;inlinecode&quot;&gt;$\theta$&lt;/span&gt;. Formally we can describe this as:&lt;/p&gt;

&lt;div&gt;
$$P(Y = 1\ |\ \theta) = \theta$$
&lt;/div&gt;

&lt;p&gt;It then follows that the probability of getting a tail is:&lt;/p&gt;

&lt;div&gt;
$$P(Y = 0\ |\ \theta) = 1 - \theta$$
&lt;/div&gt;

&lt;p&gt;These two expressions can be combined into a single expression:&lt;/p&gt;

&lt;div&gt;
$$P(Y = y\ |\ \theta) = \theta^{y}(1 - \theta)^{1 - y}$$
&lt;/div&gt;

&lt;p&gt;Notice how when &lt;span class=&quot;inlinecode&quot;&gt;$Y = 1$&lt;/span&gt;, this expression becomes &lt;span class=&quot;inlinecode&quot;&gt;$P(Y = 1 |\ \theta) = \theta$&lt;/span&gt;. And when &lt;span class=&quot;inlinecode&quot;&gt;$Y = 0$&lt;/span&gt;, this expression becomes &lt;span class=&quot;inlinecode&quot;&gt;$P(Y = 0\ |\ \theta) = 1 - \theta$&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;This particular expression is actually the probability mass function of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Bernoulli_distribution&quot;&gt;Bernoulli distribution&lt;/a&gt;. The Bernoulli distribution is used when we are describing a single trial (e.g. coin flip) with two possible outcomes (e.g. heads or tails). We can extend this to the situation where we have multiple independent trials very easily. First, we let &lt;span class=&quot;inlinecode&quot;&gt;$y_{i}$&lt;/span&gt; represent the outcome of the i&lt;sup&gt;th&lt;/sup&gt; coin flip and the set of all outcomes to be &lt;span class=&quot;inlinecode&quot;&gt;$D$&lt;/span&gt;. Since each coin toss is an independent trial (i.e. the outcome of a coin toss is independent of the previous coin toss outcomes), the probability of &lt;span class=&quot;inlinecode&quot;&gt;D&lt;/span&gt; is then multiplicative product of the individual outcomes:&lt;/p&gt;

&lt;div&gt;
$$\begin{align}
P(D\ |\ \theta) &amp;amp;= \prod_{i}{P(y_{i}\ |\ \theta)} \\
P(D\ |\ \theta) &amp;amp; = \theta^{\#heads}(1 - \theta)^{\#tails}
\end{align}$$
&lt;/div&gt;

&lt;p&gt;This expression is actually the probability mass function (pmf) for the &lt;a href=&quot;https://en.wikipedia.org/wiki/Binomial_distribution&quot;&gt;binomial distribution&lt;/a&gt;. As we are dealing with multiple coin flips here, the binomial distribution serves as the perfect model for this data. As such, we can use the pmf of the binomial distribution to be our &lt;strong&gt;likelihood function for Bayes’ rule&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;alert alert-dismissible alert-warning&quot;&gt;
&lt;h4&gt;Cool Fact&lt;/h4&gt;
The Bernoulli distribution is just a special case of the binomial distribution when there is only one trial.
&lt;/div&gt;

&lt;h3 id=&quot;step-3-specify-prior-distributions&quot;&gt;Step 3: Specify Prior Distributions&lt;/h3&gt;

&lt;p&gt;Now that we have specified a probabilistic model to represent the coin toss, we have to consider the parameters of this model. The model is comprised of a single binomial distribution pmf which is parameterized by a single &lt;span class=&quot;inlinecode&quot;&gt;$\theta$&lt;/span&gt;. So we need to define some possible values that &lt;span class=&quot;inlinecode&quot;&gt;$\theta$&lt;/span&gt; can take.&lt;/p&gt;

&lt;p&gt;For this example, we will restrict our parameter values to discrete values of &lt;span class=&quot;inlinecode&quot;&gt;$\theta = 0, \theta = 0.1, …, \theta = 1.0$&lt;/span&gt;. And we believe that certain parameters are more likely. For instance, the probability of the coin being “fair” &lt;span class=&quot;inlinecode&quot;&gt;$\theta = 0.5$&lt;/span&gt; is more likely than a coin being unfair. So we could define a probability distribution as follows:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plot_prior_distr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.prior.distr.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/how-to-bayesian-infer-101/prior-distr-triangle-1..svg&quot; alt=&quot;plot of chunk prior-distr-triangle&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is called a “prior distribution” and defines the possibilities of &lt;span class=&quot;inlinecode&quot;&gt;$\theta$&lt;/span&gt; and their associated probabilities.&lt;/p&gt;

&lt;h3 id=&quot;step-4-collect-data-and-application-of-bayes-rule&quot;&gt;Step 4: Collect Data and Application of Bayes’ Rule&lt;/h3&gt;

&lt;p&gt;The final step is that we use the observed data and apply Bayes’ rule to generate the posterior distribution. Let us start with the simplest scenario which is a single coin toss which has the outcome of a head. We now use Bayes’ rule to generate a posterior for each &lt;span class=&quot;inlinecode&quot;&gt;$\theta$&lt;/span&gt; value:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cd&quot;&gt;#' Get the Likelihood Probability Distribution
#'
#' Generates a likelihood probability distribution data frame
#'
#' @param theta.vals Vector of theta values for the binomial distribution.
#' @param num.heads Number of heads.
#' @param num.tails Number of tails.
#' @return data_frame for the likelihood probability distribution.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_likelihood_df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num.heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num.tails&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;likelihood.vals&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cur.theta.val&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;likelihood.vals&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
      &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;likelihood.vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cur.theta.val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num.heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cur.theta.val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num.tails&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;likelihood.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dplyr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                     &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;likelihood&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;likelihood.vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;likelihood.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;cd&quot;&gt;#' Get Posterior Probability Distribution
#' 
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#&quot; Generate a posterior probability distribution data.frame.
&lt;/span&gt;&lt;span class=&quot;cd&quot;&gt;#'
#' @param likelihood.df Likelihood distribution data.frame from 
#'   get_likelihood_df().
#' @param theta.prior.distr.df Prior distribution data.frame from 
#'   get_prior_distr().
#' @return data_frame of the posterior probability distribution.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_posterior_df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;likelihood.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prior.distr.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;marg.likelihood&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;likelihood.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;likelihood&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;posterior.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;likelihood.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dplyr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left_join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prior.distr.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dplyr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;marg_likelihood&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;marg.likelihood&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dplyr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;post_prob&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;likelihood&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prior&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;marg.likelihood&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;posterior.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;cd&quot;&gt;#' Plots Likelihood Probability Distribution
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_likelihood_prob_distr&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;likelihood.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;likelihood.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;likelihood&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_segment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xend&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;likelihood&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yend&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;expression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;expression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;paste&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;P(D|&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggtitle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Likelihood Distribution&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;cd&quot;&gt;#' Plots Posterior Probability Distribution
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_posterior_prob_distr&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;posterior.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;posterior.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;post_prob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_segment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xend&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;post_prob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yend&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;expression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;expression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;paste&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;P(&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;|D)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggtitle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Posterior Distribution&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;likelihood.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_likelihood_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;posterior.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_posterior_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;likelihood.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.prior.distr.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plot_grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_prior_distr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.prior.distr.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_likelihood_prob_distr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;likelihood.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_posterior_prob_distr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;posterior.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrow&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/how-to-bayesian-infer-101/posterior-prob-distr-1..svg&quot; alt=&quot;plot of chunk posterior-prob-distr&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notice how the posterior probability distribution is different from the prior distribution. Specifically, the probability mass has shifted to higher &lt;span class=&quot;inlinecode&quot;&gt;$\theta$&lt;/span&gt; values. This makes sense since that the outcome was head suggesting that the coin favours heads. Hence, &lt;span class=&quot;inlinecode&quot;&gt;$\theta$&lt;/span&gt; values supporting a bias towards head outcomes will be favoured and thus we see the mass of the posterior distribution shifting towards that direction.&lt;/p&gt;

&lt;p&gt;Importantly to note is that despite all data supporting heads, high &lt;span class=&quot;inlinecode&quot;&gt;$\theta$&lt;/span&gt; values in the posterior distribution (e.g. 0.9 and 1) do not have a lot of probability. This makes sense since a single data point should not skew our prior belief of the probability of a head. If we had more data points, we might change our beliefs. This intuition makes sense and illustrates a key concept in Bayesian inference:&lt;/p&gt;

&lt;div class=&quot;alert alert-dismissible alert-warning&quot;&gt;
&lt;h4&gt;Key Concept&lt;/h4&gt;
The posterior is a compromise between the prior and likelihood. Specifically:
&lt;ul&gt;
&lt;li&gt;When there are less data points, the distribution is skewed towards the prior distribution.&lt;/li&gt;
&lt;li&gt;When there are more data points, the distribution is skewed towards the likelihood distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;This compromise is best demonstrated with more data. Let us see what happens when we have 20 coin toss and with a total of 15 heads.&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;likelihood.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_likelihood_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;posterior.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_posterior_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;likelihood.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.prior.distr.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_prior_distr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.prior.distr.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_likelihood_prob_distr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;likelihood.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subtitle&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;15 Heads with 20 Coin Tosses&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_posterior_prob_distr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;posterior.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta.vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subtitle&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;15 Heads with 20 Coin Tosses&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrow&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;align&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;v&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/how-to-bayesian-infer-101/posterior-prob-distr-sample-size-1..svg&quot; alt=&quot;plot of chunk posterior-prob-distr-sample-size&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notice in this situation how the mass has shifted even more to the right side and in particular the majority of it is on 0.7 and 0.8. This makes sense again because the data suggest that &lt;span class=&quot;inlinecode&quot;&gt;$\theta$&lt;/span&gt; should be around 0.75. The reason why we do not see mass at 0.75 is because we restricted our parameter space to discrete values of &lt;span class=&quot;inlinecode&quot;&gt;$\theta = 0, \theta = 0.1, …, \theta = 1.0$&lt;/span&gt;. We can easily expand our parameter space to a larger “grid”:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;new.theta.vals&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new.prior.distr.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_prior_distr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new.theta.vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new.likelihood.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_likelihood_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new.theta.vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new.posterior.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_posterior_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new.likelihood.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new.prior.distr.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_prior_distr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new.prior.distr.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot.x.labels&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;FALSE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_likelihood_prob_distr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new.likelihood.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subtitle&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;15 Heads with 20 Coin Tosses&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_posterior_prob_distr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new.posterior.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new.theta.vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subtitle&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;15 Heads with 20 Coin Tosses&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrow&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/how-to-bayesian-infer-101/posterior-prob-distr-grid-1..svg&quot; alt=&quot;plot of chunk posterior-prob-distr-grid&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With more data points, we note that the posterior starts to resemble the likelihood. Let us take this one more step with 1000 data points and 750 heads:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;new.theta.vals&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new.likelihood.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_likelihood_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new.theta.vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;750&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;250&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new.posterior.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_posterior_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new.likelihood.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new.prior.distr.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_prior_distr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new.prior.distr.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot.x.labels&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;FALSE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_likelihood_prob_distr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new.likelihood.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subtitle&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;750 Heads with 1000 Coin Tosses&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_posterior_prob_distr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new.posterior.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new.theta.vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subtitle&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;750 Heads with 1000 Coin Tosses&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrow&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;align&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;v&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/how-to-bayesian-infer-101/posterior-prob-distr-grid-more-data-1..svg&quot; alt=&quot;plot of chunk posterior-prob-distr-grid-more-data&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here is the posterior is almost exactly the same as the likelihood.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;In this post, I have introduced how one can make use of Bayes’ rule to do Bayesian inference. Remember that the happens when we start to interpret the variables of Bayes’ rule as parameters (&lt;span class=&quot;inlinecode&quot;&gt;$\theta$&lt;/span&gt;) of a model and observed data (&lt;span class=&quot;inlinecode&quot;&gt;$D$&lt;/span&gt;):&lt;/p&gt;

&lt;div&gt;
$$\begin{align}
P(X\ |\ Y) &amp;amp;= \frac{P(Y\ |\ X)\ P(X)}{P(Y)} \\
P(\theta\ |\ D) &amp;amp;= \frac{P(D\ |\ \theta)\ P(\theta)}{P(D)}
\end{align}$$
&lt;/div&gt;

&lt;p&gt;I have provided an overview of the 4 key steps to any Bayesian inference, and how the posterior distribution represents a compromise between the prior and likelihood. In this example, the prior distribution was chosen arbitrary and we calculated the posterior distribution by exhaustively calculating each value. This was possible because our parameter space was small, but does not scale well to larger parameter spaces. In follow-up posts, I will discuss how we can use an analytical approach to solve this, which in turn depends on our selection of a prior distribution. Moreover, I will discuss why Bayesian statistics is difficult and how a class of methods called Markov chain Monte Carlo (MCMC) can help us deal with this!&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://sites.google.com/site/doingbayesiandataanalysis/&quot;&gt;Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;www.nature.com/nmeth/journal/v12/n5/full/nmeth.3368.html&quot;&gt;Points of significance: Bayesian statistics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;r-session&quot;&gt;R Session&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## Session info --------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;##  setting  value                       
##  version  R version 3.3.2 (2016-10-31)
##  system   x86_64, darwin11.4.2        
##  ui       unknown                     
##  language (EN)                        
##  collate  en_CA.UTF-8                 
##  tz       America/Vancouver           
##  date     2017-03-08
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## Packages ------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;##  package    * version date       source         
##  argparse   * 1.0.4   2016-10-28 CRAN (R 3.3.2) 
##  assertthat   0.1     2013-12-06 CRAN (R 3.3.2) 
##  colorspace   1.3-1   2016-11-18 CRAN (R 3.3.2) 
##  cowplot    * 0.7.0   2016-10-28 CRAN (R 3.3.2) 
##  DBI          0.5-1   2016-09-10 CRAN (R 3.3.2) 
##  devtools     1.12.0  2016-12-05 CRAN (R 3.3.2) 
##  digest       0.6.10  2016-08-02 CRAN (R 3.3.2) 
##  dplyr      * 0.5.0   2016-06-24 CRAN (R 3.3.2) 
##  evaluate     0.10    2016-10-11 CRAN (R 3.3.2) 
##  findpython   1.0.1   2014-04-03 CRAN (R 3.3.2) 
##  gdtools    * 0.1.3   2016-11-11 CRAN (R 3.3.2) 
##  getopt       1.20.0  2013-08-30 CRAN (R 3.3.2) 
##  ggplot2    * 2.2.0   2016-11-11 CRAN (R 3.3.2) 
##  gtable       0.2.0   2016-02-26 CRAN (R 3.3.2) 
##  highr        0.6     2016-05-09 CRAN (R 3.3.2) 
##  knitr      * 1.15.1  2016-11-22 CRAN (R 3.3.2) 
##  labeling     0.3     2014-08-23 CRAN (R 3.3.2) 
##  lazyeval     0.2.0   2016-06-12 CRAN (R 3.3.2) 
##  magrittr     1.5     2014-11-22 CRAN (R 3.3.2) 
##  memoise      1.0.0   2016-01-29 CRAN (R 3.3.2) 
##  munsell      0.4.3   2016-02-13 CRAN (R 3.3.2) 
##  nvimcom    * 0.9-14  2017-03-08 local (@0.9-14)
##  plyr         1.8.4   2016-06-08 CRAN (R 3.3.2) 
##  proto      * 1.0.0   2016-10-29 CRAN (R 3.3.2) 
##  R6           2.2.0   2016-10-05 CRAN (R 3.3.2) 
##  Rcpp         0.12.8  2016-11-17 CRAN (R 3.3.2) 
##  rjson        0.2.15  2014-11-03 CRAN (R 3.3.2) 
##  scales       0.4.1   2016-11-09 CRAN (R 3.3.2) 
##  stringi      1.1.2   2016-10-01 CRAN (R 3.3.2) 
##  stringr      1.1.0   2016-08-19 CRAN (R 3.3.2) 
##  svglite    * 1.2.0   2016-11-04 CRAN (R 3.3.2) 
##  tibble       1.2     2016-08-26 CRAN (R 3.3.2) 
##  withr        1.0.2   2016-06-20 CRAN (R 3.3.2)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
        <pubDate>Wed, 08 Mar 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/03/08/how-to-bayesian-infer-101.html</link>
        <guid isPermaLink="true">http://localhost:4000/2017/03/08/how-to-bayesian-infer-101.html</guid>
        
        <category>R</category>
        
        <category>stats</category>
        
        <category>bayesian</category>
        
        
      </item>
    
      <item>
        <title>Using Bioconductor in a Conda Environment</title>
        <description>&lt;p&gt;I recently switched over to using the &lt;a href=&quot;http://conda.pydata.org/docs/&quot;&gt;conda package management system&lt;/a&gt; for R. One of the benefits of conda is that package installation can be performed very easily with the command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda install -c r r-&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;name_of_package&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;One of the particular &lt;a href=&quot;http://conda.pydata.org/docs/custom-channels.html&quot;&gt;channels&lt;/a&gt; is bioconda which is focused on providing bioinformatics packages. From my understanding they are the ones responsible for uploading &lt;a href=&quot;https://www.bioconductor.org/&quot;&gt;bioconductor packages&lt;/a&gt; to the conda cloud (where all the package binaries sit).&lt;/p&gt;

&lt;p&gt;Recently, I needed to install the &lt;a href=&quot;https://www.bioconductor.org/packages/release/bioc/html/GEOquery.html&quot;&gt;bioconductor package GEOquery&lt;/a&gt; which can be installed from conda with the following command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda install -c bioconda bioconductor-geoquery
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;At the time of this writing, the current version is 2.38.4. As it turns out the NCBI, recently changed all http links to be https only resulting in this new version not working. The latest version that is on bioconductor was 2.4, and so I thought maybe the easiest thing to do was to just install it without going through conda:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;https://bioconductor.org/biocLite.R&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;biocLite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;GEOquery&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Only I ran into error:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;https://bioconductor.org/biocLite.R&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Error&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;r&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cannot&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;addition&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Warning&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;r&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;URL&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'https://bioconductor.org/biocLite.R'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;was&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Problem with the SSL CA cert (path? access rights?)'&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;It seems like the issue is that the SSL CA certificate can’t be found? A little googling revealed this &lt;a href=&quot;https://social.technet.microsoft.com/Forums/systemcenter/en-US/5d9f2b71-b1da-4006-8485-608bfab8815a/installing-bioconductor?forum=ropen&amp;lt;Paste&amp;gt;&quot;&gt;helpful post&lt;/a&gt;. Then it clued into me. I was in a conda environment (geo_test) and so the &lt;code class=&quot;highlighter-rouge&quot;&gt;https://bioconductor.org/biocLite.R&lt;/code&gt; must not be recognize that and thus not be able to find the &lt;code class=&quot;highlighter-rouge&quot;&gt;.pem&lt;/code&gt; file it needs.&lt;/p&gt;

&lt;p&gt;Searching in my conda environment revealed a &lt;code class=&quot;highlighter-rouge&quot;&gt;cacert.pem&lt;/code&gt; file to be located at:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/Users/fongchun/miniconda2/envs/geo_test/ssl/cacert.pem
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And so in R, I ran the following command:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Sys.setenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CURL_CA_BUNDLE&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/Users/fongchun/miniconda2/envs/geo_test/ssl/cacert.pem&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And then I tried to install the latest version of GEOquery from bioconductor again:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;https://bioconductor.org/biocLite.R&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;biocLite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;GEOquery&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And success! Hopefully this post helps you solve this SSL problem should you encounter this yourself when using bioconductor in a conda environment.&lt;/p&gt;

&lt;div class=&quot;alert alert-warning&quot; role=&quot;alert&quot;&gt;
&lt;h4&gt;Important Note&lt;/h4&gt;
I should note that this isn't really the &quot;correct&quot; thing to do it. I should be building the latest version and then submitted it to bioconda. But hey...I am a graduate student and I was in a time crunch :)
&lt;/div&gt;
</description>
        <pubDate>Sat, 12 Nov 2016 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2016/11/12/bioconductor-conda.html</link>
        <guid isPermaLink="true">http://localhost:4000/2016/11/12/bioconductor-conda.html</guid>
        
        <category>conda</category>
        
        <category>bioconductor</category>
        
        <category>geoquery</category>
        
        
      </item>
    
      <item>
        <title>What is the Difference Between a Hemizygous and Heterozygous Deletion?</title>
        <description>&lt;p&gt;I have been using the term hemizygous and heterozygous deletion interchangeably for the last few years to describe a single allele/copy deletion in cancer genomes. But it was recently brought to my attention that the term heterozygous deletion is not always correct when describing single copy deletions. The term heterozygous implies that the original two alleles of a genomic locus were different. But we may observe a single allele deletion where the original two alleles were identical. In this case, this would not be a heterozygous deletion, but rather it would be a hemizygous deletion which implies there is only copy remaining but makes no claim that the two original alleles were different.&lt;/p&gt;

&lt;p&gt;So basically if you are describing a single allele/copy deletion, then it is always safe to call it a hemizygous deletion. You can only call it a heterozygous deletion if you are sure that the original two alleles were actually different from each other.&lt;/p&gt;
</description>
        <pubDate>Sat, 04 Jun 2016 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2016/06/04/hemi-vs-het-del.html</link>
        <guid isPermaLink="true">http://localhost:4000/2016/06/04/hemi-vs-het-del.html</guid>
        
        <category>cancer</category>
        
        
      </item>
    
      <item>
        <title>The Basics of Survival Analysis</title>
        <description>&lt;p&gt;Survival analysis is a series of statistical methods that deals with variables that have both a time and event associated with it. For example, it is used in cancer clinical research if we are interested in measuring the time it takes before a patient relapses following treatment. In this case, the event we are measuring here is whether a patient relapses or not which has a time associated with when the relapse occurs.&lt;/p&gt;

&lt;p&gt;This will be the first of several posts on this topic. In this first post, we will introduce survival analysis and basic concepts of it:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;

&lt;ul data-toc=&quot;body&quot; data-toc-headings=&quot;h2,h3&quot;&gt;&lt;/ul&gt;

&lt;h2 id=&quot;what-is-survival-analysis&quot;&gt;What is Survival Analysis?&lt;/h2&gt;

&lt;p&gt;As mentioned in the introduction of this post, survival analysis is a series of statistical methods that deal with the outcome variable of interest being a &lt;strong&gt;time to event&lt;/strong&gt; variable. This is unlike a typical regression problem where we might be working with a continuous outcome variable (e.g. housing price) or a classification problem where we simply have a discrete variable (e.g. Class I or Class II). In survival analysis, the outcome variable has &lt;strong&gt;both a event and a time value associated with it&lt;/strong&gt;. Survival analysis is often used in medicine to study for instance a drug is able to prevent a disease from occurring (event) and how long it can say prevent it for (time). For the rest of this post, we will refer to time as survival time.&lt;/p&gt;

&lt;h2 id=&quot;censoring&quot;&gt;Censoring&lt;/h2&gt;

&lt;p&gt;One aspect that makes survival analysis difficult is the concept of censoring. &lt;strong&gt;What this means is that when a patient is censored we don’t know the true survival time for that patient&lt;/strong&gt;. There are 3 main reasons why this happens:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Individual does not experience the event when the study is over.&lt;/li&gt;
  &lt;li&gt;Individual is lost to follow-up during the study period.&lt;/li&gt;
  &lt;li&gt;Individual withdraws from the study.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There are 3 major times of censoring: right, left and interval censoring which we will discuss below.&lt;/p&gt;

&lt;h3 id=&quot;right-censored&quot;&gt;Right-censored&lt;/h3&gt;

&lt;p&gt;Right-censoring, the most common type of censoring, occurs when the survival time is “incomplete” at the right side of the follow-up period. Consider the follow example where we have 3 patients (A, B, C) enrolled onto a clinical study that runs for some period of time (study end - study start).&lt;/p&gt;

&lt;p align=&quot;middle&quot;&gt;
&lt;img src=&quot;http://localhost:4000/assets/survival-analysis/censoring/right-censoring.jpg&quot; alt=&quot;Right Censoring&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;These 3 patients have three different trajectories:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Patient A: Experiences a death before the study ends. We count this as an event.&lt;/li&gt;
  &lt;li&gt;Patient B: Survives passed the end of the study.&lt;/li&gt;
  &lt;li&gt;Patient C: Withdraws from the study.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Patient A requires no censoring since we know their exact survival time which is the time until death. Patient B however neeeds to be censored (indicated with the + at the end of the follow-up time) since we don’t know the &lt;strong&gt;exact&lt;/strong&gt; survival time of the patient; We only know that they survived up to at least the end of the study. Patient C also needs to be censored since they withdrew before the study ended. So we only know that they survived up to the time they withdrew, but again we don’t know the &lt;strong&gt;exact&lt;/strong&gt; survival time of this patient. In right censoring, the &lt;strong&gt;true survival times will always be equal to or greater than the observed survival time&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Note: This example is a bit unrealistic since it’s rare to have all patients enrolled at the same time in a study. In reality, patients would be enrolled at different times in the trial. But this example is meant more so to illustrate the concepts of censoring.&lt;/p&gt;

&lt;h3 id=&quot;left-censored&quot;&gt;Left-censored&lt;/h3&gt;

&lt;p&gt;In contrast to right-censoring, left censoring occurs when the person’s true survival time is &lt;strong&gt;less than or equal to the observed survival time&lt;/strong&gt;. An example of a situation could be for virus testing. For instance, if we’ve been following an individual and recorded an event when for instance the individual tests positive for a virus:&lt;/p&gt;

&lt;p align=&quot;middle&quot;&gt;
&lt;img src=&quot;http://localhost:4000/assets/survival-analysis/censoring/left-censoring.jpg&quot; alt=&quot;Left Censoring&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;But we don’t know the exact time of when the individual was exposed to the disease. We only know that there was some exposure between 0 and the time they were tested:&lt;/p&gt;

&lt;h3 id=&quot;interval-censored&quot;&gt;Interval-censored&lt;/h3&gt;

&lt;p&gt;Using the virus testing example, if we have the situation whether we’ve performed testing on the indvidual at some timepoint (&lt;span class=&quot;inlinecode&quot;&gt;$t_{1}$&lt;/span&gt;) and the individual was negative. But then at a timepoint further on (&lt;span class=&quot;inlinecode&quot;&gt;$t_{2}$&lt;/span&gt;), the individual tested positive:&lt;/p&gt;

&lt;p align=&quot;middle&quot;&gt;
&lt;img src=&quot;http://localhost:4000/assets/survival-analysis/censoring/interval-censoring.jpg&quot; alt=&quot;Interval Censoring&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;In this scenario, we know the individual was exposed to the virus sometime between &lt;span class=&quot;inlinecode&quot;&gt;$t_{1}$&lt;/span&gt; and &lt;span class=&quot;inlinecode&quot;&gt;$t_{2}$&lt;/span&gt;, but we do not know the exact timing of the exposure.&lt;/p&gt;

&lt;h2 id=&quot;terminology&quot;&gt;Terminology&lt;/h2&gt;

&lt;p&gt;Before going further on in this post, it’s a good time to introduce some key terminology and mathematical notation in survival analysis. The first one is:&lt;/p&gt;

&lt;dl&gt;
  &lt;dt&gt;T&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;&lt;a href=&quot;/2016/02/26/random-variables.html&quot;&gt;Random variable&lt;/a&gt; for a person’s survival time.&lt;/p&gt;
  &lt;/dd&gt;
&lt;/dl&gt;

&lt;p&gt;As T denotes time, it can take on any value between 0 to infinity. We then use:&lt;/p&gt;

&lt;dl&gt;
  &lt;dt&gt;t&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;Specific value of interest for random variable T.&lt;/p&gt;
  &lt;/dd&gt;
&lt;/dl&gt;

&lt;p&gt;So the notation, &lt;span class=&quot;inlinecode&quot;&gt;$T &amp;gt; t = 2)$&lt;/span&gt;, means we are asking whether the individual had a survival time beyond 2 months (if the unit of time is months).&lt;/p&gt;

&lt;dl&gt;
  &lt;dt&gt;d&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;Random variable indicating an event or censorship.&lt;/p&gt;
  &lt;/dd&gt;
&lt;/dl&gt;

&lt;p&gt;So for instance, we could encode events as a 1 and thus d = 1 represents the situation where an event occurs during study period. Where as d = 0, survival time is censored by end of the study.&lt;/p&gt;

&lt;p&gt;Next there are two quantitative functions which are of interest in survival analysis. These are the &lt;strong&gt;survivor function&lt;/strong&gt; and &lt;strong&gt;hazard function&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;survivor-function&quot;&gt;Survivor Function&lt;/h2&gt;

&lt;p&gt;The survivor function (aka. survival function, reliability function) is denoted as &lt;span class=&quot;inlinecode&quot;&gt;$S(t)$&lt;/span&gt;. The survivor function gives the probability that a person survives longer than some specific time t &lt;span class=&quot;inlinecode&quot;&gt;$S(t) = P(T &amp;gt; t)$&lt;/span&gt;. All survivor functions follow these same 3 characteristics:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;As t increases, the &lt;span class=&quot;inlinecode&quot;&gt;$S(t)$&lt;/span&gt; should decrease.&lt;/li&gt;
  &lt;li&gt;&lt;span class=&quot;inlinecode&quot;&gt;$S(0) = 1$&lt;/span&gt;. That is at the start of the study, no one has the event and hence the probability of surviving at &lt;span class=&quot;inlinecode&quot;&gt;$t = 0$&lt;/span&gt; is 1.&lt;/li&gt;
  &lt;li&gt;&lt;span class=&quot;inlinecode&quot;&gt;$S(\infty) = 0$&lt;/span&gt;. If the study were to go to &lt;span class=&quot;inlinecode&quot;&gt;$S(\infty)$&lt;/span&gt;, then everyone will eventually experience the event and hence the survival probability must be 0.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In theory, survival curves should be a “smooth” function with time ranging from 0 to &lt;span class=&quot;inlinecode&quot;&gt;$\infty$&lt;/span&gt;:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;dplyr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ggplot2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stat_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fun&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dexp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;S(t)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/survival-analysis/weibull_survival_function-1.png&quot; alt=&quot;plot of chunk weibull_survival_function&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, it is typical to empirically derive the survivor function from data using what is called the Kaplan-Meier method (we will cover this in an additional post). As we are often dealing with small cohorts, the survival curves oftne come out with lots of “steps”:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;survival&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ggfortify&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colon.survfit&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colon&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rx&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Obs&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;survfit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Surv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colon.survfit&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autoplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;S(t)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/survival-analysis/km_survival_function-1.png&quot; alt=&quot;plot of chunk km_survival_function&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;hazard-function&quot;&gt;Hazard Function&lt;/h2&gt;

&lt;p&gt;The other quantitative function of interest in survival analysis is the hazard function, &lt;span class=&quot;inlinecode&quot;&gt;$h(t)$&lt;/span&gt;. I always found the hazard function a bit difficult to intepret. The easiest way to think about it is to consider the scenario of where you are reading off a speedometer at a specific moment &lt;span class=&quot;inlinecode&quot;&gt;$t$&lt;/span&gt;. At this specific moment, the speed you are travelling at is 40 km/hr. What this means is if you travel at this specific rate for the next hour, you will travel 40 kilometers. But of course, there will be flucuations and you will go faster or slower than 40 km/hr so it doesn’t really give you the specific distance you will travel. So what does the 40km/hr really mean then? All it tells you is at this &lt;strong&gt;given moment you are travelling this fast.&lt;/strong&gt; Importantly, implicit to this is the fact that you have already travelled some amount of distance.&lt;/p&gt;

&lt;p&gt;The hazard function is akin to the speedometer here. Where at given moment t in time, you have this &lt;strong&gt;potential risk of having an event given you have survived up to time t.&lt;/strong&gt; Mathematically, &lt;span class=&quot;inlinecode&quot;&gt;$h(t)$&lt;/span&gt; is represented as follows:&lt;/p&gt;

&lt;div&gt;
$$h(t) = \lim_{\Delta t\to\infty} \frac{P(t \leq T &amp;lt; t + \Delta t\ |\ T \geq t)}{\Delta t}$$
&lt;/div&gt;

&lt;p&gt;Let’s break down this equation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span class=&quot;inlinecode&quot;&gt;$\lim_{\Delta t\to\infty}$&lt;/span&gt;: This indicates as the time interval approaches 0. This essentially gives us the instantaneous measurement at a particular time.&lt;/li&gt;
  &lt;li&gt;&lt;span class=&quot;inlinecode&quot;&gt;$P(t \leq T &amp;lt; t + \Delta t\ |\ T \geq t)$&lt;/span&gt;: Probability that a person’s survival time T will be between the time interval &lt;span class=&quot;inlinecode&quot;&gt;$t$&lt;/span&gt; and &lt;span class=&quot;inlinecode&quot;&gt;$t + \Delta t$&lt;/span&gt; &lt;strong&gt;given&lt;/strong&gt; that they have survived up to t (&lt;span class=&quot;inlinecode&quot;&gt;$T \geq t$&lt;/span&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So what this equation is telling us is if the time interval approaches 0 (&lt;span class=&quot;inlinecode&quot;&gt;$\lim_{\Delta t\to\infty}$&lt;/span&gt;), then we are getting the instantaneous risk of having an event at time t given they have survived up to t.&lt;/p&gt;

&lt;p&gt;It’s important to note here that the hazard is &lt;strong&gt;not a probability&lt;/strong&gt; because we are dividing the probability by a time interval. Instead what we get is a rate. And since we are dealing with the condition of survival up to time t, this is why sometimes the hazard function is referred to as &lt;strong&gt;conditional failure rate&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Unlike the &lt;span class=&quot;inlinecode&quot;&gt;$S(t)$&lt;/span&gt;, estimating the &lt;span class=&quot;inlinecode&quot;&gt;$h(t)$&lt;/span&gt; is not as simple.  One approach to estimating &lt;span class=&quot;inlinecode&quot;&gt;$h(t)$&lt;/span&gt;, is to first estimate the cumulative hazard function &lt;span class=&quot;inlinecode&quot;&gt;$H(t)$&lt;/span&gt; which is used as an intermediary to estimating &lt;span class=&quot;inlinecode&quot;&gt;$h(t)$&lt;/span&gt;. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Nelson%E2%80%93Aalen_estimator&quot;&gt;Nelson–Aalen estimator&lt;/a&gt; can be used to first estimate &lt;span class=&quot;inlinecode&quot;&gt;$H(t)$&lt;/span&gt; and then calculate the hazard function from that.&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;muhaz&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;magrittr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Uses Nelson-Aalen estimator to first get cumulative hazard, and then predict
# the hazard function from that.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colon.kphaz.fit&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colon&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rx&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Obs&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%$%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kphaz.fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;nelson&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colon.kphaz.fit&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;haz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;h(t)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/survival-analysis/hazard_function-1.png&quot; alt=&quot;plot of chunk hazard_function&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see, the &lt;span class=&quot;inlinecode&quot;&gt;$h(t)$&lt;/span&gt; is fairly erratic which is common. A “smoothing” line is often drawn to help make it more intepretable.&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;colon.kphaz.fit&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;haz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_smooth&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;h(t)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/survival-analysis/hazard_function_smooth-1.png&quot; alt=&quot;plot of chunk hazard_function_smooth&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Like &lt;span class=&quot;inlinecode&quot;&gt;$S(t)$&lt;/span&gt;, &lt;span class=&quot;inlinecode&quot;&gt;$h(t)$&lt;/span&gt; has a few key properties:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It is always non-negative.&lt;/li&gt;
  &lt;li&gt;It has no upper bound. In other words, unlike &lt;span class=&quot;inlinecode&quot;&gt;$S(t)$&lt;/span&gt;, which is a probability, the &lt;span class=&quot;inlinecode&quot;&gt;$h(t)$&lt;/span&gt; can be &amp;gt; 1 and can go up to &lt;span class=&quot;inlinecode&quot;&gt;$\infty$&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is worth mentioning that if we assume &lt;span class=&quot;inlinecode&quot;&gt;$h(t)$&lt;/span&gt; (and &lt;span class=&quot;inlinecode&quot;&gt;$S(t)$&lt;/span&gt;) follow a probability distribution then we can also esimate the functions this way. We will discuss this in a later post.&lt;/p&gt;

&lt;h2 id=&quot;survivor-function-vs-hazard-function&quot;&gt;Survivor Function vs. Hazard Function&lt;/h2&gt;

&lt;p&gt;An important thing to mention is that &lt;span class=&quot;inlinecode&quot;&gt;$S(t)$&lt;/span&gt; and &lt;span class=&quot;inlinecode&quot;&gt;$h(t)$&lt;/span&gt; are related through these two formulas:&lt;/p&gt;

&lt;div&gt;
$$\begin{align}
S(t) &amp;amp;= exp\left(-\int_{0}^{t}du\ h(u)\right) \\
h(t) &amp;amp;= -\left(\frac{dS(t)\ /\ dt}{S(t)}\right)
\end{align}$$
&lt;/div&gt;

&lt;p&gt;We can make sure of statistical computing languages (e.g. R) to make us make these transformations. For instance, the epiR package in R has the &lt;code class=&quot;highlighter-rouge&quot;&gt;epi.insthaz&lt;/code&gt; function which can transform &lt;span class=&quot;inlinecode&quot;&gt;$S(t)$&lt;/span&gt; into a &lt;span class=&quot;inlinecode&quot;&gt;$h(t)$&lt;/span&gt;:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;epiR&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colon.haz&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epi.insthaz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colon.survfit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conf.level&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.95&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colon.haz&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;est&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;h(t)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/survival-analysis/epiR_example-1.png&quot; alt=&quot;plot of chunk epiR_example&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Hopefully this first post on survival analysis gave you a good idea of some of the basic concepts in survival analysis. Ultimately, there are 3 major goals in survival analysis:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Estimate the survivor and hazard functions.&lt;/li&gt;
  &lt;li&gt;Compare survivor and/or hazard functions (e.g. log-rank test)&lt;/li&gt;
  &lt;li&gt;Assess the relationship of other variables with the survivor and hazard function. (e.g. cox regression)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will cover each of these topics in more details in subsequent posts. So stay tuned!&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.amazon.ca/Survival-Analysis-Statistics-Biology-Health-ebook/dp/B00DGEF822?ie=UTF8&amp;amp;qid=&amp;amp;ref_=tmm_kin_swatch_0&amp;amp;sr=&quot;&gt;Survival Analysis - A Self Learning Text&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/12865907&quot;&gt;Survival Analysis Part I: Basic concepts and first analyses&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stat.ethz.ch/pipermail/r-help/2004-October/058588.html&quot;&gt;Nelson-Aalen estimator in R&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stat.ethz.ch/pipermail/r-help/2009-May/390108.html&quot;&gt;Nelson-Aalen estimator of cumulative hazard&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 12 May 2016 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2016/05/12/survival-analysis.html</link>
        <guid isPermaLink="true">http://localhost:4000/2016/05/12/survival-analysis.html</guid>
        
        <category>stats</category>
        
        <category>survival</category>
        
        
      </item>
    
      <item>
        <title>Bayes' Rule</title>
        <description>&lt;p&gt;In a previous post on &lt;a href=&quot;/2016/03/20/basic-prob.html&quot;&gt;Joint, Marginal, and Conditional Probabilities&lt;/a&gt;, we learned about the 3 different types of probabilities. One famous probability rule that is built on these probabilities (specifically the conditional probability) is called “Bayes’ Rule” which forms the basis of bayesian statistics. In this post, we will learn about how to derive this rule and its utility.&lt;/p&gt;

&lt;p&gt;Here is an overview of what will be discussed in this post.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;

&lt;ul data-toc=&quot;body&quot; data-toc-headings=&quot;h2,h3&quot;&gt;&lt;/ul&gt;

&lt;h2 id=&quot;deriving-bayes-rule&quot;&gt;Deriving Bayes’ Rule&lt;/h2&gt;

&lt;p&gt;Bayes’ rules can be derived by starting with the conditional probability of &lt;span class=&quot;inlinecode&quot;&gt;$P(X\ |\ Y)$&lt;/span&gt; which is represented as:&lt;/p&gt;

&lt;div&gt;
$$P(X\ |\ Y) = \frac{P(X, Y)}{P(Y)}$$
&lt;/div&gt;

&lt;p&gt;By multiplying this with &lt;span class=&quot;inlinecode&quot;&gt;$P(Y)$&lt;/span&gt;, we get:&lt;/p&gt;

&lt;div&gt;
$$P(X\ |\ Y)\ P(Y) = P(X, Y)$$
&lt;/div&gt;

&lt;p&gt;We can also perform this same algebraic manipulation for &lt;span class=&quot;inlinecode&quot;&gt;$P(Y\ |\ X)$&lt;/span&gt;:&lt;/p&gt;

&lt;div&gt;
$$\begin{align}
P(Y\ |\ X) &amp;amp;= \frac{P(X, Y)}{P(X)} \\
P(Y\ |\ X)\ P(X) &amp;amp;= P(X, Y)
\end{align}$$
&lt;/div&gt;

&lt;p&gt;Notice how we now have two alternative representations of the joint probability &lt;span class=&quot;inlinecode&quot;&gt;$P(X, Y)$&lt;/span&gt; which we can equate to each other:&lt;/p&gt;

&lt;div&gt;
$$\begin{align}
P(X\ |\ Y)\ P(Y) &amp;amp;= P(X, Y) \\
P(Y\ |\ X)\ P(X) &amp;amp;= P(X, Y) \\
P(X\ |\ Y)\ P(Y) &amp;amp;= P(Y\ |\ X)\ P(X) 
\end{align}$$
&lt;/div&gt;

&lt;p&gt;If we now divide by &lt;span class=&quot;inlinecode&quot;&gt;$P(Y)$&lt;/span&gt;:&lt;/p&gt;

&lt;div&gt;
$$P(X\ |\ Y) = \frac{P(Y\ |\ X)\ P(X)}{P(Y)},\ if\ P(Y) \neq 0$$
&lt;/div&gt;

&lt;p&gt;This is defined as the Bayes’ rule. Essentially what we have done is relate two different, but related conditional probability equations to each other.&lt;/p&gt;

&lt;h2 id=&quot;from-prior-belief-to-updated-belief-posterior&quot;&gt;From Prior Belief to Updated Belief (Posterior)&lt;/h2&gt;

&lt;p&gt;At first glance, Bayes’ rule seems pretty simple and might not be obvious as to why it is so useful. I mean all we’ve done is just rewritten the conditional probability equation right? While that is true, the power comes from how you intepret the rule and define the variables.&lt;/p&gt;

&lt;p&gt;Let’s say we have a hypothesis (&lt;span class=&quot;inlinecode&quot;&gt;$\theta$&lt;/span&gt;) and some data (Y) to support or oppose this hypothesis. If we make our X (from above) equal to &lt;span class=&quot;inlinecode&quot;&gt;$\theta$&lt;/span&gt;, then our Bayes’ rule becomes:&lt;/p&gt;

&lt;div&gt;
$$P(\theta\ |\ Y) = \frac{P(Y\ |\ \theta)\ P(\theta)}{P(Y)}$$
&lt;/div&gt;

&lt;p&gt;So what does this mean? Well &lt;span class=&quot;inlinecode&quot;&gt;$P(\theta)$&lt;/span&gt; represents our “prior” belief of what the hypothesis is before we see any data. This prior is informed by our “expert” opinion on the hypothesis and thus is subjective. But the beauty of this rule, is that it allows us to relate this prior, &lt;span class=&quot;inlinecode&quot;&gt;$P(\theta)$&lt;/span&gt;, to an “updated belief” of the hypothesis &lt;span class=&quot;inlinecode&quot;&gt;$P(\theta\ |\ Y)$&lt;/span&gt; once we have some data to consider. In Bayesian terms, this “updated belief” is called the posterior.&lt;/p&gt;

&lt;p&gt;For the rest of this post, I will be using &lt;span class=&quot;inlinecode&quot;&gt;$P(\theta)$&lt;/span&gt; to represent the hypothesis in the Bayes’ rule. Now, let’s see this rule in action.&lt;/p&gt;

&lt;h3 id=&quot;using-bayes-rule-example-1-playing-cards&quot;&gt;Using Bayes’ Rule Example #1: Playing Cards&lt;/h3&gt;

&lt;div class=&quot;alert alert-dismissible alert-info&quot;&gt;
&lt;h4&gt;Reference&lt;/h4&gt;
This example is shamelessly borrowed from &lt;a href=&quot;https://brilliant.org/wiki/bayes-theorem/&quot;&gt;Brilliant - Bayes' Theorem and Conditional Probability&lt;/a&gt;.
&lt;/div&gt;

&lt;p&gt;Say we are playing cards and we have a prior probability that there is a &lt;span class=&quot;inlinecode&quot;&gt;$\frac{1}{13}$&lt;/span&gt; chance of getting a King card. How do we know this? Well our expert knowledge tells us that there are 52 cards in a deck with a total of 4 suits (hearts, diamonds, clubs, and jacks). Each suit has its own Jack, Queen, King (i.e. face card), and so there must be 4 Kings in total. Therefore, &lt;span class=&quot;inlinecode&quot;&gt;$\frac{4}{52} = \frac{1}{13}$&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;But now say, we also have the data that the card we have in hand is a face card. What then is the probability of getting a King now? What we really are asking is what is our updated belief (posterior), &lt;span class=&quot;inlinecode&quot;&gt;$P(H\ |\ E)$&lt;/span&gt;, in getting a King in light of this new information. We can use Bayes’ Rule to help us solve this. Let’s break down the individual components of Bayes’ rule here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span class=&quot;inlinecode&quot;&gt;$P(\theta = King) = \frac{1}{13}$&lt;/span&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;span class=&quot;inlinecode&quot;&gt;$P(Y = Face\ card)$&lt;/span&gt; = As there are 3 face cards per suit and there are 4 suits, then this is &lt;span class=&quot;inlinecode&quot;&gt;$\frac{12}{52} = \frac{3}{13}$&lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;&lt;span class=&quot;inlinecode&quot;&gt;$P(Y = Face\ card\ |\ \theta = King)$&lt;/span&gt; = Since every single King card is a face card, this probability has to be 1.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now we just fill this in into the Bayes’ Equation:&lt;/p&gt;

&lt;div&gt;
$$\begin{align}
P(\theta = King\ |\ Y = Face\ card) &amp;amp;= \frac{1 * \frac{1}{13}}{\frac{3}{13}} \\
P(\theta = King\ |\ Y = Face\ card) &amp;amp;= \frac{1}{3}
\end{align}$$
&lt;/div&gt;

&lt;p&gt;Basically we started with a prior belief, &lt;span class=&quot;inlinecode&quot;&gt;$P(\theta = King) = \frac{1}{13}$&lt;/span&gt;, but with new information regarding the card being a face card we are more confident that the card is a King now, &lt;span class=&quot;inlinecode&quot;&gt;$P(\theta = King\ |\ Y = Face\ card) = \frac{1}{3}$&lt;/span&gt;.&lt;/p&gt;

&lt;h3 id=&quot;using-bayes-rule-example-2-disease-testing&quot;&gt;Using Bayes’ Rule Example #2: Disease Testing&lt;/h3&gt;

&lt;p&gt;As another example, we will use disease testing to illustrate Bayes’ rule. Say we know that in the general populance, the chances that a person will have a certain disease is 0.001. We also know that the test has a true positive rate of 95% chance and a false positive rate of 1%. If a person gets the test and it comes back as positive, what are the chances that the person actually has the disease. Let’s first setup our variables here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span class=&quot;inlinecode&quot;&gt;$P(\theta = disease)$&lt;/span&gt;: This is our prior belief on the probability of having the disease. This is 0.001.&lt;/li&gt;
  &lt;li&gt;&lt;span class=&quot;inlinecode&quot;&gt;$P(Y = positive\ |\ \theta = disease)$&lt;/span&gt;: This is the probability of the test being positive given the person has the disease. In other words, the true positive rate which is 0.95.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;span class=&quot;inlinecode&quot;&gt;$P(Y = positive)$&lt;/span&gt;: This is the probability of the test being positive irrespective of whether the person has the disease or not. So this would include all true positive events and false positive events. So it would be summation of the two joint probabilities:&lt;/p&gt;

    &lt;div&gt;
  $$\begin{align}
  P(Y = Positive) &amp;amp;= P(Y = Positive, \theta = Disease) + P(Y = Positive, \theta \neq Disease) \\
  P(Y = Positive) &amp;amp;= 0.95 * 0.001 + 0.01 * (1-0.01) \\
  P(Y = Positive) &amp;amp;= 0.01094
  \end{align}$$
  &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So we can now use Bayes’ rule:&lt;/p&gt;

&lt;div&gt;
$$\begin{align}
P(\theta = Disease\ |\ Y = Positive) &amp;amp;= \frac{P(Y = Positive\ |\ \theta = Disease)\ P(\theta = Disease)}{P(Y = Positive)} \\
P(\theta = Disease\ |\ Y = Positive) &amp;amp;= \frac{0.95 * 0.001}{0.01094} \\
P(\theta = Disease\ |\ Y = Positive) &amp;amp;= 0.087
\end{align}$$
&lt;/div&gt;

&lt;p&gt;Notice how despite the fact that the true positive rate of the test is 0.95, the probability that the person has the disease is actually only 0.087. This is because our prior that a given person has the disease, 0.001, greatly influences this.&lt;/p&gt;

&lt;h2 id=&quot;revisiting-the-bayes-rule&quot;&gt;Revisiting the Bayes’ Rule&lt;/h2&gt;

&lt;p&gt;So far, we’ve defined the the prior probability, &lt;span class=&quot;inlinecode&quot;&gt;$P(\theta)$&lt;/span&gt;, and posterior probability, &lt;span class=&quot;inlinecode&quot;&gt;$P(\theta\ |\ Y)$&lt;/span&gt;, but the other components of the Bayes’ rule have names themselves that you should be aware of.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span class=&quot;inlinecode&quot;&gt;$P(Y\ |\ \theta)$&lt;/span&gt;: Likelihood. This is the probability of the data being generated given the hypothesis.&lt;/li&gt;
  &lt;li&gt;&lt;span class=&quot;inlinecode&quot;&gt;$P(Y)$&lt;/span&gt;: Marginal likelihood or evidence. This is just the overall probability of the data after we marginalize out all the possible hypothesis.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If we look at the Bayes’ rule again:&lt;/p&gt;

&lt;div&gt;
$$P(\theta\ |\ Y) = \frac{P(Y\ |\ \theta)\ P(\theta)}{P(Y)}$$
&lt;/div&gt;

&lt;p&gt;We can see that the posterior probability and likelihood are almost the same except for the fact the variables have been inverted. It’s important to note that these two components, in general, are NOT equivalent to each other. For instance, consider the following likelihood:&lt;/p&gt;

&lt;div&gt;
$$P(Y = Sidewalk\ is\ wet\ |\ \theta = Rained\ last\ night)$$
&lt;/div&gt;

&lt;p&gt;The probability that the sidewalk is wet given that it rained last night is pretty high. Now consider the posterior:&lt;/p&gt;

&lt;div&gt;
$$P(\theta = Rained\ last\ night\ |\ Y = Sidewalk\ is\ wet)$$
&lt;/div&gt;

&lt;p&gt;Intuitively, you should be thinking this isn’t as high because there are many reasons that could explain why the sidewalk is wet (e.g. someone spilled a drink, someone was watering the lawn).&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Here in this post we’ve shown how to derive the Bayes’ rule from the conditional probability equation. The power in the rule comes from how we intepret the variables specifically when we start thinking about it in terms of hypotheses and data to support the hypotheses. We can extend this application of Bayes’ rule to Bayesian data analysis where the hypotheses become the parameters of our model and the data is our observed data which we try to explain using the parameters of our model. This is a fairly advanced topic which we will save for another post!&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://brilliant.org/wiki/bayes-theorem/&quot;&gt;Brilliant Bayes’ Theorem and Conditional Probability&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.quora.com/What-is-the-difference-between-Bayes-Theorem-and-conditional-probability-and-how-do-I-know-when-to-apply-them&quot;&gt;What is the difference between Bayes Theorem and conditional probability and how do I know when to apply them?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.countbayesie.com/blog/2015/2/18/bayes-theorem-with-lego&quot;&gt;Count Bayesie - Bayes’ Theorem with Lego&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 21 Apr 2016 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2016/04/21/bayes-rule.html</link>
        <guid isPermaLink="true">http://localhost:4000/2016/04/21/bayes-rule.html</guid>
        
        <category>R</category>
        
        <category>stats</category>
        
        <category>bayesian</category>
        
        
      </item>
    
      <item>
        <title>Joint, Marginal, and Conditional Probabilities</title>
        <description>&lt;p&gt;Probabilities represent the chances of an event x occurring. In the classic interpretation, a probability is measured by the number of times event x occurs divided by the total number of trials; In other words, the frequency of the event occurring. There are three types of probabilities:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Joint Probabilities&lt;/li&gt;
  &lt;li&gt;Marginal Probabilities&lt;/li&gt;
  &lt;li&gt;Conditional Probabilities&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this post, we will discuss each of these probabilities. Here is an overview of what will be discussed in this post.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;

&lt;ul data-toc=&quot;body&quot; data-toc-headings=&quot;h2,h3&quot;&gt;&lt;/ul&gt;

&lt;h2 id=&quot;joint-probabilities&quot;&gt;Joint Probabilities&lt;/h2&gt;

&lt;p&gt;The first type of probability we will discuss is the joint probability which is the probability of two different events occurring at the same time. Let’s use the diamonds dataset, from ggplot2, as our example dataset. The two different variables we are interested in are diamond colors and cuts. First we will measure the frequency of each type of diamond color-cut combination. We can represent these data using a “two-way table”:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ggplot2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;dplyr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;reshape2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;knitr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diamonds.color.cut.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diamonds&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;group_by&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cut&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summarize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diamonds.color.cut.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dcast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cut&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value.nar&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;n&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;align&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;l&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;html&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table.attr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'class=&quot;table table-striped table-hover&quot;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;table class=&quot;table table-striped table-hover&quot;&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&quot;text-align:left;&quot;&gt; color &lt;/th&gt;
   &lt;th style=&quot;text-align:left;&quot;&gt; Fair &lt;/th&gt;
   &lt;th style=&quot;text-align:left;&quot;&gt; Good &lt;/th&gt;
   &lt;th style=&quot;text-align:left;&quot;&gt; Very Good &lt;/th&gt;
   &lt;th style=&quot;text-align:left;&quot;&gt; Premium &lt;/th&gt;
   &lt;th style=&quot;text-align:left;&quot;&gt; Ideal &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; D &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 163 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 662 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 1513 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 1603 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 2834 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; E &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 224 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 933 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 2400 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 2337 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 3903 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; F &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 312 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 909 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 2164 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 2331 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 3826 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; G &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 314 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 871 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 2299 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 2924 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 4884 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; H &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 303 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 702 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 1824 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 2360 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 3115 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; I &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 175 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 522 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 1204 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 1428 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 2093 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; J &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 119 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 307 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 678 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 808 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 896 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;span class=&quot;tblcaption&quot;&gt;&lt;u&gt;Table&lt;/u&gt;  1: Color-Cut Two Way Frequency Table.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Joint probabilities can be calculated by taking the proportion of times a specific color-cut combination occurs divided by total number of all color-cut combinations (i.e. frequency):&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;diamonds.color.cut.prop.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diamonds.color.cut.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ungroup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prop&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diamonds.color.cut.prop.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dcast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cut&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value.var&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;prop&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;align&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;l&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;html&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table.attr&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'class=&quot;table table-striped table-hover&quot;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;table class=&quot;table table-striped table-hover&quot;&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&quot;text-align:left;&quot;&gt; color &lt;/th&gt;
   &lt;th style=&quot;text-align:left;&quot;&gt; Fair &lt;/th&gt;
   &lt;th style=&quot;text-align:left;&quot;&gt; Good &lt;/th&gt;
   &lt;th style=&quot;text-align:left;&quot;&gt; Very Good &lt;/th&gt;
   &lt;th style=&quot;text-align:left;&quot;&gt; Premium &lt;/th&gt;
   &lt;th style=&quot;text-align:left;&quot;&gt; Ideal &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; D &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0030219 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0122729 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0280497 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0297182 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0525399 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; E &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0041528 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0172970 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0444939 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0433259 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0723582 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; F &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0057842 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0168521 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0401187 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0432147 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0709307 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; G &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0058213 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0161476 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0426214 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0542084 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0905451 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; H &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0056174 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0130145 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0338154 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0437523 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0577494 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; I &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0032443 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0096774 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0223211 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0264739 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0388024 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; J &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0022062 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0056915 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0125695 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0149796 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0166110 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;span class=&quot;tblcaption&quot;&gt;&lt;u&gt;Table&lt;/u&gt;  2: Color-Cut Two Way Probability Table.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Based on &lt;u&gt;Table&lt;/u&gt;  2, we can see that certain color-cut combinations are more probable than others.&lt;/p&gt;

&lt;div class=&quot;alert alert-dismissible alert-info&quot;&gt;
&lt;h4&gt;Heads Up!&lt;/h4&gt;
For brevity and simpler mathematical notions, for the rest of this post we will use the random variable X to represent color and Y to represent cut.
&lt;/div&gt;

&lt;p&gt;For instance, a diamond with the X = G and Y = cut , &lt;span class=&quot;inlinecode&quot;&gt;$P(X = G, Y= ideal) = 0.09$&lt;/span&gt;, is much more probable than a diamond with X = D and Y = fair, &lt;span class=&quot;inlinecode&quot;&gt;$P(X = D, Y = fair) = 0.003$&lt;/span&gt;.&lt;/p&gt;

&lt;h2 id=&quot;marginal-probabilities&quot;&gt;Marginal Probabilities&lt;/h2&gt;

&lt;p&gt;The second type of probability is the marginal probability. The interesting thing about a marginal probability is that the term sounds complicated, but it’s actually the probability that we are most familiar with. Basically anytime you are in interested in a single event irrespective of any other event (i.e. “marginalizing the other event”), then it is a marginal probability. For instance, the probability of a coin flip giving a head is considered a marginal probability because we aren’t considering any other events. Typically, we just say probability and not the marginal part of it because this part only comes into play when we have to factor in a second event.&lt;/p&gt;

&lt;p&gt;To illustrate this, we will go back to our diamond color-cut combination two-way table (&lt;u&gt;Table&lt;/u&gt;  2). If we are interested in say the marginal probabilty &lt;span class=&quot;inlinecode&quot;&gt;$P(X = D)$&lt;/span&gt;, then basically we are asking “what is the probability of getting a diamond that is color D irrespective of its cut?” It should be initutive that we can calculate this information by simply summing up the joint probabilities of the row color D. Mathematically:&lt;/p&gt;

&lt;div&gt;
$$P(X = D) = \sum_{y \in S_{Y}}P(X = D, Y = y)$$
&lt;/div&gt;

&lt;p&gt;Where &lt;span class=&quot;inlinecode&quot;&gt;$S_{Y}$&lt;/span&gt; represents all the possible values of the random variable Y. In other words, we are holding X constant (&lt;span class=&quot;inlinecode&quot;&gt;$X = D$&lt;/span&gt;) while iterating over all the possible Y values and summing up the joint probabilities.&lt;/p&gt;

&lt;p&gt;We can calculate the marginal probability of all the different colors. We can also calculate the marginal probability of cut by using the same logic and summing up the joint probabilities of the columns. For instance, to calculate &lt;span class=&quot;inlinecode&quot;&gt;$P(Y = Fair)$&lt;/span&gt;,&lt;/p&gt;

&lt;div&gt;
$$P(Y = Fair) = \sum_{x \in S_{X}}P(X = x, Y = Fair)$$
&lt;/div&gt;

&lt;p&gt;Let’s add the marginal probabilities to the two way table now:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;color.marginal.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diamonds.color.cut.prop.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;group_by&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summarize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;marginal&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cut.marginal.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diamonds.color.cut.prop.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;group_by&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cut&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summarize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;marginal&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diamonds.color.cut.prop.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dcast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cut&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value.var&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;prop&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left_join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color.marginal.df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;by&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;color&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bind_rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cut.marginal.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;marginal&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dcast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cut&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value.var&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;marginal&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;align&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;l&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;html&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table.attr&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'class=&quot;table table-striped table-hover&quot;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;table class=&quot;table table-striped table-hover&quot;&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&quot;text-align:left;&quot;&gt; color &lt;/th&gt;
   &lt;th style=&quot;text-align:left;&quot;&gt; Fair &lt;/th&gt;
   &lt;th style=&quot;text-align:left;&quot;&gt; Good &lt;/th&gt;
   &lt;th style=&quot;text-align:left;&quot;&gt; Very Good &lt;/th&gt;
   &lt;th style=&quot;text-align:left;&quot;&gt; Premium &lt;/th&gt;
   &lt;th style=&quot;text-align:left;&quot;&gt; Ideal &lt;/th&gt;
   &lt;th style=&quot;text-align:left;&quot;&gt; marginal &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; D &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0030219 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0122729 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0280497 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0297182 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0525399 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.1256025 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; E &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0041528 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0172970 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0444939 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0433259 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0723582 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.1816277 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; F &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0057842 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0168521 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0401187 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0432147 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0709307 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.1769003 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; G &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0058213 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0161476 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0426214 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0542084 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0905451 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.2093437 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; H &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0056174 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0130145 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0338154 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0437523 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0577494 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.1539488 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; I &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0032443 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0096774 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0223211 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0264739 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0388024 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.1005191 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; J &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0022062 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0056915 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0125695 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0149796 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0166110 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0520578 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; marginal &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0298480 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.0909529 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.2239896 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.2556730 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; 0.3995365 &lt;/td&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; NA &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;span class=&quot;tblcaption&quot;&gt;&lt;u&gt;Table&lt;/u&gt;  3: Color-Cut Two Way Probability Table with Marginal Probabilities. The marginal probability of each cut is represented in the last row whereas the marginal probability of each color ir represented in the last column.&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;conditional-probability&quot;&gt;Conditional Probability&lt;/h2&gt;

&lt;p&gt;The final type of probability is the conditional probability. A conditional probability is the probability of an event X occurring when a secondary event Y is true. Mathematically, it is represented as &lt;span class=&quot;inlinecode&quot;&gt;$P(X \ |\ Y)$&lt;/span&gt;. This is read as “probability of X given/conditioned on Y”.&lt;/p&gt;

&lt;p&gt;For example, if someone asked you the probability of getting a diamond with the G color, &lt;span class=&quot;inlinecode&quot;&gt;$P(X = G)$&lt;/span&gt;, we can use &lt;u&gt;Table&lt;/u&gt;  3 to find the marginal probability of this event. But what if you had an additional layer of information where you knew that the diamond was also of ideal cut? This becomes a conditional probability since we have an event that is already true. A conditional probability can be calculated as follows:&lt;/p&gt;

&lt;div&gt;
$$P(X\ |\ Y) = \frac{P(X, Y)}{P(Y)}$$
&lt;/div&gt;

&lt;p&gt;Recall that the marginal probability is simply summing up the joint probabilities while holding one variable constant. So we can further breakdown this equation as follows:&lt;/p&gt;

&lt;div&gt;
$$P(X\ |\ Y) = \frac{P(X, Y)}{\sum_{x \in S_{X}}P(X = x, Y)}$$
&lt;/div&gt;

&lt;p&gt;So for us to work this out for our particular question, we need two pieces of information:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;span class=&quot;inlinecode&quot;&gt;$P(Y = ideal)$&lt;/span&gt;: Marginal probability of Y = ideal.&lt;/li&gt;
  &lt;li&gt;&lt;span class=&quot;inlinecode&quot;&gt;$P(X = G, Y = ideal)$&lt;/span&gt;: Joint probability of X = G and Y = ideal.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So we can calculate the conditional probability as follows:&lt;/p&gt;

&lt;div&gt;
$$P(X = G\ |\ Y = ideal) = \frac{P(X = G, Y = ideal)}{\sum_{x \in S_{X}}P(X = x, Y = ideal)}$$
&lt;/div&gt;

&lt;p&gt;So the conditional probability would be in this case:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;joint.prob&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diamonds.color.cut.prop.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;G&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cut&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Ideal&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prop&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;marg.prob&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cut.marginal.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cut&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Ideal&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;marginal&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cond.prob&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;joint.prob&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;marg.prob&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cond.prob&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## [1] 0.2266252
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;So basically if we didn’t factor in any other information, our &lt;span class=&quot;inlinecode&quot;&gt;$P(X = G)$&lt;/span&gt; was
0.2093437.
But once we factored in an additional level of information which was Y = ideal, our probability changed to 0.2266252. Put another way, we had a “reallocation of our belief” in an event once we factored in additional information.&lt;/p&gt;

&lt;h2 id=&quot;defining-a-joint-probability-equation&quot;&gt;Defining a Joint Probability Equation&lt;/h2&gt;

&lt;p&gt;In the conditional and marginal probabilities section, we defined the mathematical equations for them. We can now define a mathematical equation for joint probabilities which actually uses both the conditional and marginal probability equations. Starting with the conditional probability equation, we can do a bit of algebraic manipulation for defining joint probabilities now:&lt;/p&gt;

&lt;div&gt;
$$\begin{align}
P(X\ |\ Y) &amp;amp;= \frac{P(X, Y)}{P(Y)} \\
P(X\ |\ Y)\ P(Y) &amp;amp;= P(X, Y) \\
P(X\ |\ Y)\ \sum_{x \in S_{X}}P(X = x, Y) &amp;amp;= P(X, Y) \\
\end{align}$$
&lt;/div&gt;

&lt;h2 id=&quot;what-about-continuous-random-variables&quot;&gt;What about Continuous Random Variables?&lt;/h2&gt;

&lt;p&gt;In this post’s example dataset of diamonds, we used the random variables X and Y to represent diamond colors and cuts respectively. Both of which are discrete random variables. If dealing with continuous random variables, these probabilities still exist with the exception that we are dealing with integrals instead of summations. For instance, the mathematical representation of marginal probabilities for continuous variables becomes an integral:&lt;/p&gt;

&lt;div&gt;
$$P(X = D) = \int_{}P(X = D, Y = y)\ dY$$
&lt;/div&gt;

&lt;h2 id=&quot;frequentist-vs-bayesian-view&quot;&gt;Frequentist vs. Bayesian View&lt;/h2&gt;

&lt;p&gt;One last thing worth mentioning is that in introduction of this post I made a statement regarding the “classic interpretation” of probability. Specifically this “classic interpretation” is referred to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Frequentist_probability&quot;&gt;frequentist view of probability&lt;/a&gt;. In this view, probabilities are based purely on objective, random experiments with the assumption that given enough trials (“long run”) the relative frequency of event x will equal to the true probability of x. Notice how all of the probabilities we reported in this post were based purely on the frequency.&lt;/p&gt;

&lt;p&gt;If you’ve done any statistics or analytics, you’ll likely have come across the term “&lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian_statistics&quot;&gt;bayesian statistics&lt;/a&gt;”. In brief, bayesian statistics differs from the frequentists view in that it incorporates subjective probability which is the “degree of belief” in an event. This degree of belief is called the “prior probability distribution” and is incorporated along with the data from random experiments when determining probabilities. Bayesian statistics will be discussed in a separate post.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Frequentist_probability&quot;&gt;Frequentist Probability - Wikipedia&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://sites.nicholas.duke.edu/statsreview/probability/jmc/&quot;&gt;Nicholas School Statistics Review - Joint, Marginal and Conditional Probabilities&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://sites.google.com/site/doingbayesiandataanalysis/&quot;&gt;Chapter 4 - Doing Bayesian Data Analysis - A Tutorial with R, JAGS, and Stan&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://setosa.io/ev/conditional-probability/&quot;&gt;Explained Visually - Conditional probability&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;r-session&quot;&gt;R Session&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## Session info --------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;##  setting  value                       
##  version  R version 3.2.2 (2015-08-14)
##  system   x86_64, darwin13.4.0        
##  ui       unknown                     
##  language (EN)                        
##  collate  en_CA.UTF-8                 
##  tz       America/Vancouver           
##  date     2016-03-22
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## Packages ------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;##  package    * version    date       source        
##  argparse   * 1.0.1      2014-04-05 CRAN (R 3.2.2)
##  assertthat   0.1        2013-12-06 CRAN (R 3.2.2)
##  captioner  * 2.2.3.9000 2015-09-16 local         
##  colorspace   1.2-6      2015-03-11 CRAN (R 3.2.2)
##  DBI          0.3.1      2014-09-24 CRAN (R 3.2.2)
##  devtools     1.9.1      2015-09-11 CRAN (R 3.2.2)
##  digest       0.6.9      2016-01-08 CRAN (R 3.2.2)
##  dplyr      * 0.4.3      2015-09-01 CRAN (R 3.2.2)
##  evaluate     0.8        2015-09-18 CRAN (R 3.2.2)
##  findpython   1.0.1      2014-04-03 CRAN (R 3.2.2)
##  formatR      1.2.1      2015-09-18 CRAN (R 3.2.2)
##  getopt       1.20.0     2013-08-30 CRAN (R 3.2.2)
##  ggplot2    * 2.0.0      2015-12-18 CRAN (R 3.2.2)
##  gtable       0.1.2      2012-12-05 CRAN (R 3.2.2)
##  highr        0.5.1      2015-09-18 CRAN (R 3.2.2)
##  knitr      * 1.12.7     2016-02-09 local         
##  lazyeval     0.1.10     2015-01-02 CRAN (R 3.2.2)
##  magrittr     1.5        2014-11-22 CRAN (R 3.2.2)
##  memoise      0.2.1      2014-04-22 CRAN (R 3.2.2)
##  munsell      0.4.3      2016-02-13 CRAN (R 3.2.2)
##  plyr         1.8.3      2015-06-12 CRAN (R 3.2.2)
##  proto      * 0.3-10     2012-12-22 CRAN (R 3.2.2)
##  R6           2.1.2      2016-01-26 CRAN (R 3.2.2)
##  Rcpp         0.12.3     2016-01-10 CRAN (R 3.2.2)
##  reshape2   * 1.4.1      2014-12-06 CRAN (R 3.2.2)
##  rjson        0.2.15     2014-11-03 CRAN (R 3.2.2)
##  scales       0.3.0      2015-08-25 CRAN (R 3.2.2)
##  stringi      1.0-1      2015-10-22 CRAN (R 3.2.2)
##  stringr      1.0.0      2015-04-30 CRAN (R 3.2.2)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 20 Mar 2016 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2016/03/20/basic-prob.html</link>
        <guid isPermaLink="true">http://localhost:4000/2016/03/20/basic-prob.html</guid>
        
        <category>R</category>
        
        <category>stats</category>
        
        
      </item>
    
      <item>
        <title>Probability Distributions and their Mass/Density Functions</title>
        <description>&lt;p&gt;A probability distribution is a way to represent the possible values and the respective probabilities of a random variable. There are two types of probability distributions: discrete and continuous probability distribution. As you might have guessed, a discrete probability distribution is used when we have a discrete random variable. A continuous probability distribution is used when we have a continuous random variable.&lt;/p&gt;

&lt;p&gt;In this post, we will explore what discrete and continuous probability distributions are. Additionally, we will describe what a probability mass and density function, their key properties, and how they relate to probability distributions. Here is an overview of what will be discussed in this post.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;

&lt;ul data-toc=&quot;body&quot; data-toc-headings=&quot;h2,h3&quot;&gt;&lt;/ul&gt;

&lt;h2 id=&quot;discrete-probability-distributions&quot;&gt;Discrete Probability Distributions&lt;/h2&gt;

&lt;p&gt;In my previous post on &lt;a href=&quot;/2016/02/26/random-variables.html&quot;&gt;random variables&lt;/a&gt;, I used the example of a random process that involved flipping a coin x number of times and measuring the total number of heads using a discrete random variable X. As an example, let us try to build a probability distribution from a random process like this where we are flipping a coin 3 times. This random process can have a total of 8 possible outcomes:&lt;/p&gt;

&lt;div class=&quot;alert alert-dismissible alert-warning&quot;&gt;
&lt;h4&gt;Heads Up!&lt;/h4&gt;
[I suggest watching this video](https://www.youtube.com/watch?v=5lpqiGixDd0) if you are unclear on how these outcomes were generated.
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;HHH&lt;/li&gt;
  &lt;li&gt;HHT&lt;/li&gt;
  &lt;li&gt;HTH&lt;/li&gt;
  &lt;li&gt;HTT&lt;/li&gt;
  &lt;li&gt;THH&lt;/li&gt;
  &lt;li&gt;THT&lt;/li&gt;
  &lt;li&gt;TTH&lt;/li&gt;
  &lt;li&gt;TTT&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We let our random variable Y serve as a way to map the number of heads we get to a numeric value. So the most initutive way would be:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;X = 0 if we get no heads.&lt;/li&gt;
  &lt;li&gt;X = 1 if we get 1 head.&lt;/li&gt;
  &lt;li&gt;X = 2 if we get 2 heads.&lt;/li&gt;
  &lt;li&gt;X = 3 if we get 3 heads.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So now that we have this random variable and the possible values this variable can take, let’s try to figure out the associated probabilities of each outcome.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;span class=&quot;inlinecode&quot;&gt;$P(X = 0)$&lt;/span&gt;: The only way we can get 0 heads is if all 3 coin flips gives a tail. The only outcome that satisfies this is the TTT outcome. This means the probability &lt;span class=&quot;inlinecode&quot;&gt;$P(X = 0) = \frac{1}{8}$&lt;/span&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;span class=&quot;inlinecode&quot;&gt;$P(X = 1)$&lt;/span&gt;: To get 1 head, the outcomes HTT, THT, and TTH satisfy this. So this means &lt;span class=&quot;inlinecode&quot;&gt;$P(X = 1) = \frac{3}{8}$&lt;/span&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;span class=&quot;inlinecode&quot;&gt;$P(X = 2)$&lt;/span&gt;: To get 2 heads, the outcomes HHT, HTH, and THH satisfy this. So this means &lt;span class=&quot;inlinecode&quot;&gt;$P(X = 2) = \frac{3}{8}$&lt;/span&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;span class=&quot;inlinecode&quot;&gt;$P(X = 3)$&lt;/span&gt;: The only way we can get 3 heads is if all 3 coin flips gives a head The only outcome that satisfies this is the TTT outcome. This means the probability &lt;span class=&quot;inlinecode&quot;&gt;$P(X = 3) = \frac{1}{8}$&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;What we have just described is called a “&lt;strong&gt;probability mass function (pmf)&lt;/strong&gt;” which is a function,  &lt;span class=&quot;inlinecode&quot;&gt;$f(X)$&lt;/span&gt;, that defines the probability of the discrete random variable X taking on a particular value x. When we take all the possible values (sample space) and associated probabilities into consideration, it is called a discrete probability distribution (as defined by a pmf). We can visualize this particular pmf as follows:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ggplot2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;dplyr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prob.distr.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                            &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prob&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prob.distr.df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stat&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;identity&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;X (Number of Heads)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;f(X) Probability&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/prob-distr/discrete_prob_distr_example-1.svg&quot; alt=&quot;plot of chunk discrete_prob_distr_example&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here we have a discrete probability distribution of the random variable Y. The x-axes shows the different outcomes of the random variable while the y-axes shows the corresponding probabilities of these outcomes&lt;/p&gt;

&lt;p&gt;You might sometimes see the term probability distribution table. This is just the same thing as a pmf. The name stems from the fact that there are a finite number of outcomes and and so we can represent these outcomes and their associated probabilities in a finite table. As we will see below, we can’t do this for a continuous random variable hence why a probabilty distribution table only has meaning in the context of a discrete random variable.&lt;/p&gt;

&lt;h2 id=&quot;continuous-probability-distributions&quot;&gt;Continuous Probability Distributions&lt;/h2&gt;

&lt;p&gt;In the example above, X was a discrete random variable. When the outcomes are discrete we have the ability to directly measure the probability of each outcome. When the random variable is continuous, then things get a little more complicated. &lt;strong&gt;We are not able to directly measure the probability of a specific continuous value&lt;/strong&gt;. This may seem a bit confusing at first, but imagine you had a random variable, Y, that measured the price of a diamond. Now what if someone asked you the following question: If you sampled a single diamond, what is the probability that its &lt;strong&gt;exact price is $326.57&lt;/strong&gt;? Not $326.58 or $326.56, but exactly $326.57?&lt;/p&gt;

&lt;p&gt;If you think of it that way, then the probability of getting a diamond with that exact price is probably really low. In fact, the probability of any exact price is really low. &lt;strong&gt;This is why the concept of probability for a given value when the value is on a continous scale doesn’t make sense&lt;/strong&gt;. Instead, what we do is “discretize” the sample space so that we can work in intervals instead of individual values. To make this more concrete, we will use the diamond dataset from ggplot2 to illustrate this example. First, let’s plot a summary histogram of the diamond prices for 53940 diamonds and use an interval size of 100 (to represent $100 intervals):&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;diamonds&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;price&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_histogram&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binwidth&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Y (Diamond Price)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Number of Diamonds&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/prob-distr/diamond_hist-1.svg&quot; alt=&quot;plot of chunk diamond_hist&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once we have these intervals of data, we can start talking about proportion of samples falling into intervals For instance, we can ask the question what is the probability of a diamond having a price between $1000 and $1100:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;num.diamonds.in.bin&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diamonds&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;price&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;price&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;A total of 1832 diamonds fall in this interval which equates to the following proportion of all diamonds in the dataset:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;prob.mass&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num.diamonds.in.bin&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diamonds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prob.mass&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## [1] 0.03396366
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;When we talk about the proportion of outcomes falling into an interval like this, then this is called a “probability mass”. As the probability mass is dependent on the interval size, the “probability density” is used to represent the ratio of the probability mass to interval size:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;prob.dens&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prob.mass&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prob.dens&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## [1] 0.0003396366
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;To get more precision, we would want our intervals to be small since wide intervals are not very informative. Ideally, our intervals should be infinitesimally small. When we do this, we produce something that starts to resemble a “curve” (here we use &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_density_estimation&quot;&gt;kernel density estimation&lt;/a&gt; to estimate the curve).&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;diamonds&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;price&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_density&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Y (Diamond Price)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;f(Y) Density&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/prob-distr/diamond_pdf-1.svg&quot; alt=&quot;plot of chunk diamond_pdf&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This “curve”, &lt;span class=&quot;inlinecode&quot;&gt;$f(Y)$&lt;/span&gt;, is called a probability density function (pdf) which is used to describe the probability distribution of a continuous random variable.&lt;/p&gt;

&lt;h2 id=&quot;properties-of-probability-massdensity-functions&quot;&gt;Properties of Probability Mass/Density Functions&lt;/h2&gt;

&lt;p&gt;There are a few key properites of a pmf, &lt;span class=&quot;inlinecode&quot;&gt;$f(X)$&lt;/span&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;span class=&quot;inlinecode&quot;&gt;$f(X = x) &amp;gt; 0$&lt;/span&gt; where &lt;span class=&quot;inlinecode&quot;&gt;$x \in S_{X}$&lt;/span&gt; (&lt;span class=&quot;inlinecode&quot;&gt;$S_{X}$&lt;/span&gt; = sample space of X).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since we can directly measure the probability of an event for discrete random variables, then&lt;/p&gt;

    &lt;div&gt;
 $$P(X = x) = f(X = x)$$
 &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The probability of all possible events must sum to 1:&lt;/p&gt;

    &lt;div&gt;
 $$\sum_{x \in S_{X}} f(X) = 1$$
 &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The key properites of a pdf, &lt;span class=&quot;inlinecode&quot;&gt;$f(Y)$&lt;/span&gt;, are very similar to a pmf. The big difference is that &lt;strong&gt;we need to think in terms of intervals instead of individual outcomes&lt;/strong&gt;. This means we have to work with integrals and not summations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;span class=&quot;inlinecode&quot;&gt;$f(Y = y) &amp;gt; 0$ where &lt;span class=&quot;inlinecode&quot;&gt;$y \in S_{Y}$&lt;/span&gt; (&lt;span class=&quot;inlinecode&quot;&gt;$S_{Y}$&lt;/span&gt; = sample space of Y). This property is the same as for a pmf.&lt;/span&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The probability of a value &lt;span class=&quot;inlinecode&quot;&gt;$y \in [a, b]$&lt;/span&gt; is:&lt;/p&gt;

    &lt;div&gt;
 $$P(a \leq y \leq b) = \int_{a}^{b} f(Y) \,dy$$
 &lt;/div&gt;

    &lt;p&gt;In other words, the probability of &lt;span class=&quot;inlinecode&quot;&gt;$y \in [a, b]$&lt;/span&gt; is equivalent to taking the integral of the pdf between a and b.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The entire area under the pdf must sum to 1.&lt;/p&gt;

    &lt;div&gt;
 $$\int_{-\infty}^{\infty} f(Y) \,dy = 1$$
 &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;types-of-probability-massdensity-functions&quot;&gt;Types of Probability Mass/Density Functions&lt;/h2&gt;

&lt;p&gt;You may have heard the phrase that random variable “Y follows a [insert name of] distribution”. What this means is that they are assuming the data being generated comes from a particular well studied distribution. In statistics, there exists many discrete (e.g. binomial, negative binomial) and continuous (e.g. gaussian, student-t) distributions that have been well studied. So by making the assumption that a random variable follows a particular distribution, we can use the distribution’s derived pmf/pdf and established properties to help us answer questions about the data. For instance, we know that the pdf of a gaussian is:&lt;/p&gt;

&lt;div&gt;
$$f(Y \,|\, \mu, \sigma^{2}) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x - \mu)^{2}}{2\sigma^{2}}}$$
&lt;/div&gt;

&lt;p&gt;It is parameterized by a mean &lt;span class=&quot;inlinecode&quot;&gt;$\mu$&lt;/span&gt; and a standard deviation &lt;span class=&quot;inlinecode&quot;&gt;$\sigma$&lt;/span&gt; while being symmetrical around the mean. A standard gaussian (&lt;span class=&quot;inlinecode&quot;&gt;$\mu = 0, \sigma = 1$&lt;/span&gt;) would look like this.&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;set.seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vals&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rnorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# dnorm provides the probability density
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stat_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fun&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dnorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Density&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/prob-distr/std_normal_distr-1.svg&quot; alt=&quot;plot of chunk std_normal_distr&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Just like before, we can ask questions like &lt;span class=&quot;inlinecode&quot;&gt;$P(-2 \leq y \leq 2)$&lt;/span&gt; and use the gaussian pdf to answer this question:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# pnorm provides the probability 
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pnorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pnorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## [1] 0.9544997
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pnorm(y)&lt;/code&gt; function is essentially taking the integral from &lt;span class=&quot;inlinecode&quot;&gt;$-\infty$&lt;/span&gt; to &lt;span class=&quot;inlinecode&quot;&gt;$y$&lt;/span&gt;:&lt;/p&gt;

&lt;div&gt;
$$\int_{-\infty}^{y} f(Y) \,dy$$
&lt;/div&gt;

&lt;p&gt;So what we are doing here is:&lt;/p&gt;

&lt;div&gt;
$$\int_{-2}^{2} f(Y) \,dy = \int_{-\infty}^{2} f(Y) \,dy - \int_{-\infty}^{-2} f(Y) \,dy$$
&lt;/div&gt;

&lt;p&gt;This effectively calculates the area between -2 and 2:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# dnorm provides the probability density
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stat_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fun&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dnorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Density&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_vline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xintercept&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linetype&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;dotted&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;red&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_vline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xintercept&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linetype&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;dotted&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;red&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/prob-distr/std_normal_distr_integral-1.svg&quot; alt=&quot;plot of chunk std_normal_distr_integral&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One caveat of this approach is that our assumption could be wrong. &lt;strong&gt;This means that one needs to be careful when deciding what type of probability distribution a random variable follows&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Thankfully there are some well established principles that can help us. The figure below provides a decision tree that gives you an idea of some common probability distributions that one can use given the data they have in hand:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/prob-distr/overview-prob-distr.png&quot; alt=&quot;Overview of Common Probability Distributions&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is Figure 6A.15 (Pg 61) from &lt;a href=&quot;http://people.stern.nyu.edu/adamodar/pdfiles/papers/probabilistic.pdf&quot;&gt;Probabilistic approaches to risk by Aswath Damodaran&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Hopefully this post sheds a bit of light on what a probability distribution and how we can describe them using probability mass/density functions. The big take home messages are as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A probability distribution is a way to represent the possible values and the respective probabilities of a random variable. There are two types of probability distributions:
    &lt;ul&gt;
      &lt;li&gt;Discrete probability distribution for discrete random variables.&lt;/li&gt;
      &lt;li&gt;Continuous probability distribution for continuous random variables.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;We can directly calculate probabilites of a discrete random variable, X = x, as the proportion of times the x value occurs in the random process.&lt;/li&gt;
  &lt;li&gt;Probabilites of a continuous random variable taking on a specific value (e.g. Y = y) are &lt;strong&gt;not directly measureable&lt;/strong&gt;. Instead, we calculate the probability as the proportion of times &lt;span class=&quot;inlinecode&quot;&gt;$y \in [a, b]$&lt;/span&gt;.&lt;/li&gt;
  &lt;li&gt;Probability mass functions (pmf) are used to describe discrete probability distributions. While probability density functions (pdf) are used to describe continuous probability distributions.&lt;/li&gt;
  &lt;li&gt;By assuming a random variable follows an established probability distribution, we can use its derived pmf/pdf and established principles to answer questions we have about the data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://sites.google.com/site/doingbayesiandataanalysis/&quot;&gt;Doing Bayesian Data Analysis - A Tutorial with R, JAGS, and Stan&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=5lpqiGixDd0&quot;&gt;Probability Distribution Table - Intro with tossing a coin 3 times&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://stattrek.com/probability-distributions/probability-distribution.aspx&quot;&gt;What is a Probability Distribution?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://stattrek.com/statistics/dictionary.aspx?definition=Continuous%20probability%20distribution&quot;&gt;Continuous Probability Distribution&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Fvi9A_tEmXQ&quot;&gt;Khan Academy - Probability density function&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://onlinecourses.science.psu.edu/stat414/node/97&quot;&gt;PennState STAT 414/415 - Probability Density Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.quora.com/What-is-the-relationship-between-the-probability-mass-density-and-cumulative-distribution-functions&quot;&gt;What is the relationship between the probability mass, density, and cumulative distribution functions?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;r-session&quot;&gt;R Session&lt;/h2&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;devtools&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;session_info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## Session info --------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;##  setting  value                       
##  version  R version 3.2.2 (2015-08-14)
##  system   x86_64, darwin13.4.0        
##  ui       unknown                     
##  language (EN)                        
##  collate  en_CA.UTF-8                 
##  tz       America/Vancouver           
##  date     2016-03-18
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## Packages ------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;##  package    * version    date       source        
##  argparse   * 1.0.1      2014-04-05 CRAN (R 3.2.2)
##  assertthat   0.1        2013-12-06 CRAN (R 3.2.2)
##  captioner  * 2.2.3.9000 2015-09-16 local         
##  colorspace   1.2-6      2015-03-11 CRAN (R 3.2.2)
##  DBI          0.3.1      2014-09-24 CRAN (R 3.2.2)
##  devtools     1.9.1      2015-09-11 CRAN (R 3.2.2)
##  digest       0.6.9      2016-01-08 CRAN (R 3.2.2)
##  dplyr      * 0.4.3      2015-09-01 CRAN (R 3.2.2)
##  evaluate     0.8        2015-09-18 CRAN (R 3.2.2)
##  findpython   1.0.1      2014-04-03 CRAN (R 3.2.2)
##  formatR      1.2.1      2015-09-18 CRAN (R 3.2.2)
##  getopt       1.20.0     2013-08-30 CRAN (R 3.2.2)
##  ggplot2    * 2.0.0      2015-12-18 CRAN (R 3.2.2)
##  gtable       0.1.2      2012-12-05 CRAN (R 3.2.2)
##  knitr      * 1.12.7     2016-02-09 local         
##  labeling     0.3        2014-08-23 CRAN (R 3.2.2)
##  lazyeval     0.1.10     2015-01-02 CRAN (R 3.2.2)
##  magrittr   * 1.5        2014-11-22 CRAN (R 3.2.2)
##  memoise      0.2.1      2014-04-22 CRAN (R 3.2.2)
##  munsell      0.4.3      2016-02-13 CRAN (R 3.2.2)
##  plyr         1.8.3      2015-06-12 CRAN (R 3.2.2)
##  proto      * 0.3-10     2012-12-22 CRAN (R 3.2.2)
##  R6           2.1.2      2016-01-26 CRAN (R 3.2.2)
##  Rcpp         0.12.3     2016-01-10 CRAN (R 3.2.2)
##  rjson        0.2.15     2014-11-03 CRAN (R 3.2.2)
##  scales       0.3.0      2015-08-25 CRAN (R 3.2.2)
##  stringi      1.0-1      2015-10-22 CRAN (R 3.2.2)
##  stringr      1.0.0      2015-04-30 CRAN (R 3.2.2)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
        <pubDate>Thu, 17 Mar 2016 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2016/03/17/prob-distr.html</link>
        <guid isPermaLink="true">http://localhost:4000/2016/03/17/prob-distr.html</guid>
        
        <category>R</category>
        
        <category>stats</category>
        
        
      </item>
    
      <item>
        <title>Do You Understand Random Variables?</title>
        <description>&lt;p&gt;A random variable can be a confusing concept because it is not like a &lt;strong&gt;traditional variable&lt;/strong&gt; that you may have been exposed to before. I will be honest and say that I never really understood them until I started doing some more digging into them. This post hopes to clarify exactly what a random variable. Here is an overview of what will be discussed in this post.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;

&lt;ul data-toc=&quot;body&quot; data-toc-headings=&quot;h2,h3&quot;&gt;&lt;/ul&gt;

&lt;h2 id=&quot;what-exactly-is-a-random-variable&quot;&gt;What Exactly is a Random Variable?&lt;/h2&gt;

&lt;p&gt;A random variable is actually a function that map the outcomes of a random process to a numeric value. For instance, let’s say we have the random process of flipping a coin where the coin can only have one of two outcomes - head or a tail. We could use the random variable X (random variables are typically denoted with capital letters) to represent the outcomes of a coin flip. Here is one possible random variable:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;X = 1 if the flip of the coin is a head&lt;/li&gt;
  &lt;li&gt;X = 0 if the flip of the coin is a tail&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There isn’t anything special about the numeric values I chose here. I could have done the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;X = 500 if the flip of the coin is a head&lt;/li&gt;
  &lt;li&gt;X = 25 if the flip of the coin is a tail&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And this would still be a random variable. But the numeric values in the first random variable are more intuitive of course. As another example, let’s say we have a random process of flipping a coin 5 times. We can assign a random variable X that maps the number of heads we get from the 5 coin flips to numeric values:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;X = 0 if no heads&lt;/li&gt;
  &lt;li&gt;X = 1 if 1 head&lt;/li&gt;
  &lt;li&gt;X = 2 if 2 heads&lt;/li&gt;
  &lt;li&gt;X = 3 if 3 heads&lt;/li&gt;
  &lt;li&gt;X = 4 if 4 heads&lt;/li&gt;
  &lt;li&gt;X = 5 if 5 heads&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;alert alert-dismissible alert-info&quot;&gt;
&lt;h4&gt;Why is it a Random Process?&lt;/h4&gt;
The flipping of a coin is a random process because each flip of the coin can produce either a heads or a tail and is essentially a random event. If we had some weird biased coin that had both sides being head, then the flipping of this coin would be a deterministic process since we would always get a head every flip.
&lt;/div&gt;

&lt;h2 id=&quot;why-random-variables&quot;&gt;Why Random Variables?&lt;/h2&gt;

&lt;p&gt;Now that we have defined what a random variable is, the next logical question is why are we even doing this? The major reason for declaring random variables is because it provides us a way to ask questions about the random process in a concise mathematical way. If we go back to the random process of flipping 5 coins and use the random variable X to map the number of heads we get, then we can easily represent questions about the process like this:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;span class=&quot;inlinecode&quot;&gt;$P(X = 2)$&lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;&lt;span class=&quot;inlinecode&quot;&gt;$P(X &amp;lt; 4)$&lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;&lt;span class=&quot;inlinecode&quot;&gt;$P(X &amp;gt; 1)$&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These 3 questions equate to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;What is the probability of getting exactly 2 heads?&lt;/li&gt;
  &lt;li&gt;What is the probability of getting less than 4 heads?&lt;/li&gt;
  &lt;li&gt;What is the probability of getting more than 1 head?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If we didn’t do it this way, the alternative to represent these questions could have been something like this:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;P(Probability of getting exactly 2 heads when we flip a coin 5 times)&lt;/li&gt;
  &lt;li&gt;P(Probability of getting &amp;lt; 4 heads when we flip a coin 5 times)&lt;/li&gt;
  &lt;li&gt;P(Probability of getting &amp;gt; 1 head when we flip a coin 5 times)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;discrete-vs-continuous-random-variables&quot;&gt;Discrete vs. Continuous Random Variables&lt;/h2&gt;

&lt;p&gt;The other important thing that is worth mentioning is distinguishing between a discrete and a continuous random variable. In the examples mentioned above, the numeric values that the random variable mapped to were all discrete values (0, 1, 2, 3, 4, 5). This makes all the above random variables discrete random variables. This is contrasted to a continuous random variable that maps outcomes to continuous numeric values.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Hopefully this post sheds a bit of light on what random variables are and why we use them. In some follow-up posts (e.g. probability distributions), the utility of random variables will hopefully become even more clear. So stay tuned!&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://youtu.be/3v9w79NhsfI&quot;&gt;Random variables - Probability and Statistics - Khan Academy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/206/what-is-the-difference-between-discrete-data-and-continuous-data&quot;&gt;What is the difference between discrete data and continuous data?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 26 Feb 2016 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2016/02/26/random-variables.html</link>
        <guid isPermaLink="true">http://localhost:4000/2016/02/26/random-variables.html</guid>
        
        <category>stats</category>
        
        <category>randvar</category>
        
        
      </item>
    
      <item>
        <title>Installing topicmodels - &quot;fatal error: 'gsl/gsl_rng.h'&quot;</title>
        <description>&lt;p&gt;I recently tried to install the &lt;a href=&quot;https://cran.r-project.org/web/packages/topicmodels/index.html&quot;&gt;topicmodels R package&lt;/a&gt; (v0.2-3) on my Mac that was running OS X Yosemite (v10.10.4 - 14E46) with Xcode (v6.4 - 6E35b). My R (v3.2.2) was installed using homebrew.&lt;/p&gt;

&lt;p&gt;I ran the following command in my R console:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;install.packages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;topicmodels&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Only to be met with the following errors:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ctm.c:29:10: fatal error: 'gsl/gsl_rng.h' file not found
#include &amp;lt;gsl/gsl_rng.h&amp;gt;
         ^
1 error generated.
make: *** [ctm.o] Error 1
fatal error: 'gsl/gsl_rng.h'
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The topicmodels R package actually depends on the &lt;a href=&quot;http://www.gnu.org/software/gsl/&quot;&gt;GNU scientific library (GSL)&lt;/a&gt;. You can get this using homebrew:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;brew install gsl
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;At the time of this writing, this version was stable 1.16 (bottled). But even after installing this, I still got the same error. After taking a closer look, at the include statements:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;-I/usr/local/opt/gettext/include -I/usr/local/opt/readline/include -I/usr/local/opt/openssl/include
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;I noticed this wasn’t actually including the gsl headers located at &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/local/opt/gsl/include&lt;/code&gt;. A post about this exact same problem was made on &lt;a href=&quot;http://stackoverflow.com/questions/24172188/how-can-i-install-topicmodels-package-in-r&quot;&gt;stackoverflow&lt;/a&gt;. The solution was to include the following in &lt;code class=&quot;highlighter-rouge&quot;&gt;~/.R/Makevars&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;PKG_LIBS=-L/usr/local/opt/gettext/lib
CFLAGS=-I/usr/local/opt/gsl/include
LDFLAGS=-L/usr/local/opt/gsl/lib -lgsl -lgslcblas
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;More details on what a Makevars file is can be found &lt;a href=&quot;http://r-pkgs.had.co.nz/src.html&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote class=&quot;blockquote&quot;&gt;
&lt;p&gt;makevars is a make file that overrides the default make file generated by R (which is located at file.path(R.home(&quot;etc&quot;), &quot;Makeconf&quot;))&lt;/p&gt;
&lt;small&gt;Hadley Wickham in &lt;cite title=&quot;Source Title&quot;&gt;R packages&lt;/cite&gt;&lt;/small&gt;
&lt;/blockquote&gt;

&lt;p&gt;How this helps you if you run into the same problem!&lt;/p&gt;

&lt;h2 id=&quot;for-centos-users&quot;&gt;For Centos Users&lt;/h2&gt;

&lt;p&gt;For those on Centos, you install gsl (if you have admin):&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yum install gsl-devel
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This will install the headers, by default into &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/include/gsl&lt;/code&gt;.&lt;/p&gt;
</description>
        <pubDate>Sat, 20 Feb 2016 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2016/02/20/install-r-topicmodels.html</link>
        <guid isPermaLink="true">http://localhost:4000/2016/02/20/install-r-topicmodels.html</guid>
        
        <category>R</category>
        
        <category>topicmodels</category>
        
        
      </item>
    
      <item>
        <title>Configuring R - &quot;cannot compile a simple Fortran program&quot;</title>
        <description>&lt;p&gt;I recently had to install an older version of R (v3.1.2) from source for a specific project. Even though, I have done this a few dozen times it never ceases to amaze me that I still run into new errors. While trying to run &lt;code class=&quot;highlighter-rouge&quot;&gt;configure&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./configure --prefix&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/home/fong/usr/local/R/3.1.2
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;I ran into this error message:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.
.
.
checking for Fortran 77 libraries of f95...
checking how to get verbose linking output from gcc... -v
checking for C libraries of gcc...  -L/usr/local/lib64 -L/usr/lib64/../lib64 -L/usr/lib/../lib64 -L/usr/local/lib/gcc/x86_64-unknown-linux-gnu/5.2.0 -L/usr/local/lib/gcc/x86_64-unkn
own-linux-gnu/5.2.0/../../../../lib64 -L/lib/../lib64 -L/usr/lib64 -L/usr/local/lib/gcc/x86_64-unknown-linux-gnu/5.2.0/../../../../x86_64-unknown-linux-gnu/lib -L/usr/local/lib/gcc/
x86_64-unknown-linux-gnu/5.2.0/../../.. -lgcc_s
checking for dummy main to link with Fortran 77 libraries... none
checking for Fortran 77 name-mangling scheme... configure: error: in `/home/fong/R-3.2.3':
configure: error: cannot compile a simple Fortran program
See `config.log' for more details
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;After doing some googling (&lt;a href=&quot;https://github.com/Homebrew/homebrew/issues/12776&quot;&gt;reference&lt;/a&gt;), it appeared the issue had to do with no fortran 77 installed on my computer.&lt;/p&gt;

&lt;p&gt;But when I checked:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;which gfortran
/usr/local/bin/gfortran
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Clearly, it was on my system. After looking into the &lt;code class=&quot;highlighter-rouge&quot;&gt;configure&lt;/code&gt; parameters, I found that there was a &lt;code class=&quot;highlighter-rouge&quot;&gt;F77&lt;/code&gt; parameter that allows me to set “Fortran 77 compiler command”. So I modified my &lt;code class=&quot;highlighter-rouge&quot;&gt;configure&lt;/code&gt; command to:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./configure --prefix&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/home/fong/usr/local/R/3.1.2 &lt;span class=&quot;nv&quot;&gt;F77&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;gfortran
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This fixed my configure problem and the rest of the installation process went smoothly.&lt;/p&gt;
</description>
        <pubDate>Wed, 13 Jan 2016 11:41:00 +0800</pubDate>
        <link>http://localhost:4000/2016/01/13/configure-r-cannot-compile-fortran.html</link>
        <guid isPermaLink="true">http://localhost:4000/2016/01/13/configure-r-cannot-compile-fortran.html</guid>
        
        <category>R</category>
        
        
      </item>
    
  </channel>
</rss>
